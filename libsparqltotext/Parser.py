import argparse

# Connections options
PROVIDER = "SERVER"
SERVER_ADDR = "127.0.0.1"
SERVER_PORT = "8080"
SERVER_COMPLETION_ENDPOINT = "/completion"
CT_MODEL_PATH = ""
CT_CONTEXT_LENGTH = 4096
 
# Dataset processing options
STARTING_ROW_OFFSET = 0
NUMBER_OF_ROWS_TO_PROCESS = 0 # Rows [STARTING_ROW_OFFSET:STARTING_ROW_OFFSET+NUMBER_OF_ROWS_TO_PROCESS-1] will be processed. <0 will do [STARTING_ROW_OFFSET:len(data)-1].
SYSTEM_PROMPT = "<<SYS>>This is a conversation between User and Llama, a friendly chatbot. Llama is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.<</SYS>>\n"
MAX_NUMBER_OF_TRY_PER_PROMPT = 25
PREPARE_PROMPTS = "auto"

# Dataset output options
OUTPUT_PATH = "./outputs/generated_prompts/"
QUERIES_PATH = "./datasets/final_queries_v1.1.json"

# Prompt processing/Generation options
NUMBER_OF_TOKEN_TO_PREDICT = 256
TEMPERATURE = 0.4 # (default = 0.8)

SAVE_ID = "0"
SAVE_CHECKPOINT_PATH = "./outputs/checkpoints/"

def parse_script_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument("-pv", "--provider", type=str, help=f"Who completes the answer (default={PROVIDER}).", choices=["SERVER", "CTRANSFORMERS"], default=PROVIDER)
    parser.add_argument("-saddr", "--server-address", type=str, help=f"IP address or URL of the server that has the LLM API endpoint if the provider is SERVER (default={SERVER_ADDR}).", default=SERVER_ADDR)
    parser.add_argument("-sport", "--server-port", type=str, help=f"Port to ask for connection with the server_address if the provider is SERVER (default={SERVER_PORT}).", default=SERVER_PORT)
    parser.add_argument("-e", "--endpoint", type=str, help=f"Endpoint of the API if the provider is SERVER (default={SERVER_COMPLETION_ENDPOINT}).", default=SERVER_COMPLETION_ENDPOINT)
    parser.add_argument("-mp", "--model-path", type=str, help=f"Path to the model if the provider is CTRANSFORMERS (default={CT_MODEL_PATH}).", default=CT_MODEL_PATH)
    parser.add_argument("-nctx", "--context-length", type=int, help=f"Size of the Context length of the model (default={CT_CONTEXT_LENGTH}).", default=CT_CONTEXT_LENGTH)
    
    parser.add_argument("-o", "--offset", type=int, help=f"Offset the starting row processed by the script. (default={STARTING_ROW_OFFSET})", default=STARTING_ROW_OFFSET)
    parser.add_argument("-n", "--number-of-rows", type=int, help=f"Rows [offset:offset+n-1] will be processed. <0 will do [offset:number of rows in the data-1] (default={NUMBER_OF_ROWS_TO_PROCESS}), will process all rows).", default=NUMBER_OF_ROWS_TO_PROCESS)
    parser.add_argument("-r", "--retry-attempts", type=int, help=f"Number of retries to attempt on generating prompts from one row query. A failure happen when the result from the LLM does not satisfy the constraints. 0 means no retry, -1 means retry until constraint satisfaction, n positive tells the number of attempts (default={MAX_NUMBER_OF_TRY_PER_PROMPT}).", default=MAX_NUMBER_OF_TRY_PER_PROMPT)
    parser.add_argument("-pp", "--prepare-prompts", type=str, help=f"Should the script prepare prompts? \"auto\" will detect if there is a \"prompt\" column in the dataset. \"no\" will not do anything, and \"yes\" will create a prompt column and make them (default={PREPARE_PROMPTS}).", choices=["auto", "yes", "no"], default=PREPARE_PROMPTS)
    
    parser_system_prompt = parser.add_mutually_exclusive_group()
    parser_system_prompt.add_argument("-sys", "--system-prompt", type=str, help=f"The system prompt to use, a default system prompt is automatically given. (default={SYSTEM_PROMPT})", default=SYSTEM_PROMPT)
    parser_system_prompt.add_argument("-sysp", "--system-prompt-path", type=str, help=f"Path to the system prompt file which should be a normal text file, a default system prompt is automatically given. (default={SYSTEM_PROMPT})", default="")
    
    parser.add_argument("-out", "--output-path", type=str, help=f"Path to the directory where to output file (default={OUTPUT_PATH}).", default=OUTPUT_PATH)
    parser.add_argument("-p", "--queries-path", type=str, help=f"Path to the queries' file (default={QUERIES_PATH}).", default=QUERIES_PATH)
    
    parser.add_argument("-np", "--prediction-size", type=int, help=f"Define the number of token maximum generated by the LLM. The LLM can try to match this given size. Higher number might give more accurate result (default={NUMBER_OF_TOKEN_TO_PREDICT}).", default=NUMBER_OF_TOKEN_TO_PREDICT)
    parser.add_argument("-t", "--temperature", type=float, help=f"Temperature is a parameter of randomness of the output generated by the LLM, check google for more information (default={TEMPERATURE}).", default=TEMPERATURE)
    
    parser_verbosity = parser.add_mutually_exclusive_group()
    parser_verbosity.add_argument("-q", "--quiet", action="store_true" ,help=f"Disable any output from the script.")
    parser_verbosity.add_argument("-v", "--verbose", action="store_true" ,help=f"Show more information about the process.")
    
    parser.add_argument("-pa", "--print-answers", action="store_true" ,help=f"Print answers from the LLM.")
    parser.add_argument("-pr", "--print-results", action="store_true" ,help=f"Print results extracted from the answer of LLM.")
    
    parser.add_argument("-id", "--save-identifier", type=str, help=f"Save ID, used to make checkpoint and resume at checkpoints (default={SAVE_ID})", default=SAVE_ID)
    parser.add_argument("-cp", "--checkpoint-path", type=str, help=f"Path to save checkpoint (default={SAVE_CHECKPOINT_PATH})", default=SAVE_CHECKPOINT_PATH)
    
    return parser.parse_args()