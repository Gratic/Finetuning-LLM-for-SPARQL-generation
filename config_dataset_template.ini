[Execution]
random_seed = 123456

[Scripts]
templatize_script = scripts/templatize_queries.py
query_execution_script = scripts/execute_queries.py
split_dataset_script = scripts/generate_finetune_dataset.py

[Dataset]
dataset_path = datasets/final_queries_v1.8.json
; dataset_path = scripts/templatize_test.json

[Prompt Generation]

    [Prompt Generation.Using Basic]
    prefix = basic_
    query_column = query

    [Prompt Generation.Using Templated]
    prefix = templated_
    query_column = query_templated

    [Prompt Generation.Provider Configuration]
    # Possible provider are SERVER, CTRANSFORMERS, LLAMACPP, VLLM, TRANSFORMERS, TRANSFORMERSv2, OpenAI
    provider = LLAMACPP
    # Required for SERVER, LLAMACPP
    server_address = 127.0.0.1
    server_port = 8080
    completion_endpoint = /completion
    tokenizer_endpoint = /tokenize
    # Required for CTRANSFORMERS, VLLM, TRANSFORMERS, TRANSFORMERSv2
    model_path = 
    context_length = 4096
    api_key=

    [Prompt Generation.Dataset Configuration]
    # Generation types are continuous, targeted, skipped
    generation = continuous
    offset = 0
    # Number of rows tell the number of rows to generate prompt for
    # a number <= 0 will tell to do the whole dataset.
    number_of_rows = 0
    target_rows =
    # Retry attempts: A positive number will make the script try to generate a prompt X times.
    # A negative number will make the script try until success.
    retry_attempts = -1

    [Prompt Generation.Generation Configuration]
    # Prompt to give to the llm to make it generate prompts...
    prepare_prompts = yes
    # Comma separated list of row to process, works only with generation = targeted
    prompt_template = [INST] [data] [system_prompt] [/INST]
    system_prompt = Read QUERY, DESCRIPTION, and CONTEXT. There is a machine capable of writing the given SparQL QUERY if we provide the correct prompt. A prompt should be at least one sentence and no longer than five sentences. Each prompt must be enclosed in quotation marks. Provide a list of three prompts that would elicit the QUERY.
    prompt =
    leading_answer_prompt =
    prediction_size = 1024
    temperature = 0.2

[Query Execution]
timeout = 300
# If per_query_answer_limit == 0, no LIMIT clause will be automatically added, however if present already will not be removed.
per_query_answer_limit = 10

[Provider LLAMACPP]
launch_server = True
# Delay to wait for the server to launch in seconds
delay = 60
server_path = ../llama.cpp/server
model_path = ../mistral-7b-instruct-v0.2.Q5_K_M.gguf
n_layer_on_gpu = 33
context_length = 2048