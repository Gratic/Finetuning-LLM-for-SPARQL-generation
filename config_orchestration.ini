[Execution]
# Use Huggingface's Accelerate library
use_accelerate = False

# If Do_Preprocess_Gold is True, the script will execute the queries in the gold dataset
# and then proceed to make the preprocessing necessary to speed up the evaluation
# of the LLMs. Otherwise, if False, will use the Preprocess_Gold_Path to load an already
# preprocessed gold dataset.
do_preprocess_gold = False
preprocess_gold_path =

[Model Information]
# Path is the path to find the model or the huggingface's id [company]/[model name].
path = mistralai/Mistral-7B-Instruct-v0.2
name = Mistral-7B-Instruct-v0.2
context_length = 4096

[Datasets]
# Paths to the different splits of the datasets.
# Must be done before using this scripts using generate_finetune_dataset.py
train = outputs/finetune_dataset_train.pkl
valid = outputs/finetune_dataset_valid.pkl
test = outputs/finetune_dataset_test.pkl

[Training Hyperparameters]
lora_r_value = [4, 8]
lora_dropout = [0, 0.05]
batch_size = [1]
packing = [0]
neft_tune_alpha = [0, 1, 5]
epochs = 3
pipeline_type = [template] # Possible values are defined in Pipeline Types To Target Columns section under.

[Evaluation Hyperparameters]
# Possible engine are vllm and peft.
engine = peft
temperature = 0.2
top_p = 0.95

[Training Hyperparameters Name Abbreviations]
lora_r_value = rv
lora_dropout = ld
batch_size = bs
packing = p
neft_tune_alpha = nta
num_epochs = e

[Evaluation Hyperparameters Name Abbreviations]
engine = eng
pipeline = pip
temperature = t
top-p = topp

[Pipeline Types To Target Columns]
# Each key is a possible pipeline_type value of the section Training Hyperparameters.
# Different pipelines needs different target (not preprocessed the same).
# Here we define, which pipeline needs which target column.
template = target_template
basic = target_raw