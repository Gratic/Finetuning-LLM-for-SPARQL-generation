[Execution]
# Use Huggingface's Accelerate library
use_accelerate = False

# If Do_Preprocess_Gold is True, the script will execute the queries in the gold dataset
# and then proceed to make the preprocessing necessary to speed up the evaluation
# of the LLMs. Otherwise, if False, will use the Preprocess_Gold_Path to load an already
# preprocessed gold dataset.
do_preprocess_gold = True
preprocess_gold_path = 
random_seed = 123456

[Models]
# Path is the path to find the model or the huggingface's id [company]/[model name].
models = [
    {
        "path": "mistralai/Mistral-7B-Instruct-v0.2",
        "name": "Mistral-7B-Instruct-v0.2",
        "context_length": 4096
    }
    ]

[Datasets]
# Paths to the different splits of the datasets.
# Must be done before using this scripts using generate_finetune_dataset.py
train = outputs/finetune_dataset_train.pkl
valid = outputs/finetune_dataset_valid.pkl
test = outputs/finetune_dataset_test.pkl

[Training Hyperparameters]
# lora_r_value, lora_dropout, batch_size, packing, neft_tune_alpha and pipeline_type are comma separated list.
# e.g. lora_r_value = 4, 8 will be interpreted as ["4", "8"].
# A cross product is realized between above mentioned list to train with all possible combinations.
# Packing in particular needs to be 0 or 1 exclusively, because it gets treated as a boolean.
lora_r_value = 4, 8, 16, 32
lora_dropout = 0, 0.05
batch_size = 1
packing = 0
neft_tune_alpha = 0, 1, 5
epochs = 3
# Possible values are defined in Pipeline Types To Target Columns section under.
pipeline_type = basic

[Evaluation Hyperparameters]
# Possible engine are vllm and peft.
engine = peft
temperature = 0.2
top_p = 0.95

[Training Hyperparameters Name Abbreviations]
lora_r_value = rv
lora_dropout = ld
batch_size = bs
packing = p
neft_tune_alpha = nta
num_epochs = e

[Evaluation Hyperparameters Name Abbreviations]
engine = eng
pipeline = pip
temperature = t
top_p = topp

[Pipeline Types To Target Columns]
# Each key is a possible pipeline_type value of the section Training Hyperparameters.
# Different pipelines needs different target (not preprocessed the same).
# Here we define, which pipeline needs which target column.
template = target_template
basic = target_raw
