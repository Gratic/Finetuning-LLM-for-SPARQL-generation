[Execution]
# Launch the training script using Huggingface's Accelerate library.
use_accelerate = False

# Used to set up all random seeds of the pipeline.
random_seed = 123456

[Models]
# path: where to find the model, can be a folder or a hugging face id
# name: name of the model for creating files and for logging in WandB
# context_length: the maximal context length to use, higher value increases memory consumption
# token: auth token for gated models
# quantization: how to quantisize the models weight when fine tuning, possible options are 'no', '4bit' and '8bit'. By default set to '4bit'.
; {
;     "path": "meta-llama/Llama-2-7b-chat-hf",
;     "name": "Llama-2-7b-chat-hf",
;     "context_length": 512,
;     "token": "hf_Dycuxxxxxxxxxxxxxxxxxxx",
;     "quantization": "4bit"
; }
# models is a list of dictionary represented above. It must be a well-formatted JSON file.
models = [
    {
        "path": "mistralai/Mistral-7B-Instruct-v0.2",
        "name": "Mistral-7B-Instruct-v0.2",
        "context_length": 4096
    }
    ]

[Datasets]
# Must be done before using this scripts using generate_finetune_dataset.py
hf_dataset = Zaleks/labelized_sparql_wikidata

[Training Hyperparameters]
# lora_r_value, lora_dropout, batch_size, packing, neft_tune_alpha and pipeline_type are comma separated list.
# e.g. lora_r_value = 4, 8 will be interpreted as ["4", "8"].
# A cross product is realized between above mentioned list to train with all possible combinations.

# Optimizer should be the name (or names separated by a comma) of huggingface's implemented optimizer names.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
# https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.optim
optimizer=adamw_torch
# computational_datatype choices are fp16, bf16, fp32. Controls the datatype during the forward and backward pass.
computational_datatype = fp16
learning_rate = 0.00002
# lora r value is the LoRA Rank Value which decide the size of the adapters
lora_r_value = 4, 8, 16, 32
# lora r alpha is computed with lora_r_value, r alpha = lora_r_value * lora_r_alpha_mult
lora_r_alpha_mult = 1
lora_dropout = 0, 0.05
batch_size = 1
# packing is a boolean (0 = False, 1 = True)
packing = 0
# neft tune alpha is turned off when equal to 0
neft_tune_alpha = 0
gradient_accumulation = 4
# gradient_checkpointing is a boolean (0 = False, 1 = True)
gradient_checkpointing = 1
epochs = 3
# Possible values are defined in Pipeline Types To Target Columns section under.
pipeline_type = basic
# Possible values are defined in Input Types to Input Columns section under.
input_type = basic
# Should the model do evaluation ?
do_eval = True

[Evaluation Hyperparameters]
# Possible engine are vllm and peft.
engine = peft
# decoding can be 'greedy' or 'sampling' with peft engine
decoding = greedy
temperature = 0.2
top_p = 0.95
num_tokens=1024
# computational_type changes the type of the weight of the model during evaluation (works only with engine=peft). Choices are 'fp32', 'fp16' and 'bf16'.
computational_type=bf16
start_tag =[sparql]
end_tag =[/sparql]

[Training Hyperparameters Name Abbreviations]
optimizer = opt
computational_datatype = cd
learning_rate = lr
lora_r_value = rv
lora_r_alpha_mult = ramul
lora_dropout = ld
batch_size = bs
packing = p
neft_tune_alpha = nta
gradient_accumulation = ga
gradient_checkpointing = gc
num_epochs = e

[Evaluation Hyperparameters Name Abbreviations]
engine = eng
decoding = d
temperature = t
top_p = topp
num_tokens = no
computational_type=cd
start_tag = no
end_tag = no

[Pipeline Types To Target Columns]
# Each key is a possible pipeline_type value of the section Training Hyperparameters.
# Different pipelines needs different target (not preprocessed the same).
# Here we define, which pipeline needs which target column.
template = target_template
basic = target_raw

[Input Types to Input Columns]
template = templated_input
basic = basic_input
