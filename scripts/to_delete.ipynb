{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "import pandas as pd\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/batch_run/faith_template5/execution/Mistral-7B-Instruct-v0.2_rv16-ld0.05-bs1-p0-nta1-e3-template_engpeft-t0.2-topp0.95_executed.parquet.gzip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0].translated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0].linked_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0].target_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "print(f\"{tokenizer.unk_token=}\")\n",
    "print(f\"{tokenizer.pad_token=}\")\n",
    "print(f\"{tokenizer.unk_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "print(f\"{tokenizer.unk_token=}\")\n",
    "print(f\"{tokenizer.pad_token=}\")\n",
    "print(f\"{tokenizer.unk_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/batch_run/democracy_template8/execution/Mistral-7B-Instruct-v0.2_rv32-ld0.05-bs1-p0-nta1-e3-template_engpeft-t0.2-topp0.95_executed.parquet.gzip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handpicked_data = df.loc[(~df['execution'].str.startswith(\"exception:\")) & (~df['execution'].isnull()) & (df['execution'].map(len) > 0)].iloc[[1, 2, 3, 206, 207]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tokenizer.batch_encode_plus([f\"`sparql\\n{i}`\" for i in handpicked_data['translated_prompt'].to_list()], add_special_tokens=False, padding='longest', return_tensors='np')['input_ids'].tolist()\n",
    "labels = tokenizer.batch_encode_plus([f\"`sparql\\n{i}`\" for i in handpicked_data['target_template'].to_list()], add_special_tokens=False, padding='longest', return_tensors='np')['input_ids'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data = {\n",
    "    \"preds\": preds,\n",
    "    \"labels\": labels,\n",
    "}\n",
    "\n",
    "path = Path(\"sft_peft_compute_metrics_execute_ok.json\")\n",
    "path.write_text(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "\n",
    "\n",
    "df = load_dataset(\"../outputs/batch_run/lezgo/execution/Mistral-7B-Instruct-v0.2_rv16-ld0-bs1-p0-nta0-e3-template_engpeft-t0.2-topp0.95_executed.parquet.gzip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "df_out = df.head()\n",
    "\n",
    "path = Path(\"evaluation_test.json\")\n",
    "path.write_text(df_out.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_gold = json.loads(Path(\"../outputs/batch_run/lezgo/preprocessed_gold.json\").read_text())\n",
    "df_gold = pd.read_json(data_gold['df_gold_eval'])\n",
    "df_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold_out = df_gold.head()\n",
    "\n",
    "path = Path(\"evaluation_test_gold.json\")\n",
    "path.write_text(df_gold_out.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"../datasets/final_queries_v1.7.json\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = dataset[2549:2553]\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out_v2 = df_out.reset_index(drop=True)\n",
    "df_out_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"templatize_test.json\")\n",
    "path.write_text(df_out_v2.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/dataset_pipeline/fq17_3/fq17_3-split_test.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = df.iloc[0]\n",
    "for c in df.columns:\n",
    "    print(c)\n",
    "    print(entry[c])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/dataset_pipeline/fq17_3/fq17_3-split_test.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2].target_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments, AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sft_peft import format_prompt\n",
    "import sft_peft\n",
    "from prompts_template import BASE_MISTRAL_TEMPLATE, BASE_BASIC_INSTRUCTION\n",
    "from libwikidatallm.TemplateLLMQuerySender import TemplateLLMQuerySender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=\"[/INST]\", tokenizer=tokenizer)\n",
    "datasets = load_dataset(\"pandas\", data_files=\n",
    "                          {\n",
    "                              \"valid\": \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_valid.pkl\",\n",
    "                              \"train\": \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_train.pkl\",\n",
    "                              \"test\": \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_test.pkl\",\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    valid: Dataset({\n",
       "        features: ['basic_input', 'templated_input', 'target_raw', 'target_template', 'index'],\n",
       "        num_rows: 129\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['basic_input', 'templated_input', 'target_raw', 'target_template', 'index'],\n",
       "        num_rows: 1923\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['basic_input', 'templated_input', 'target_raw', 'target_template', 'index'],\n",
       "        num_rows: 513\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from SFTTrainer(Trainer) _prepare_non_packed_dataloader method\n",
    "def _prepare_non_packed_dataloader(\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    dataset_text_field,\n",
    "    max_seq_length,\n",
    "    formatting_func=None,\n",
    "    add_special_tokens=True,\n",
    "    remove_unused_columns=True,\n",
    "):\n",
    "    use_formatting_func = formatting_func is not None and dataset_text_field is None\n",
    "    # self._dataset_sanity_checked = False\n",
    "\n",
    "    # Inspired from: https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt\n",
    "    def tokenize(element):\n",
    "        outputs = tokenizer(\n",
    "            element[dataset_text_field] if not use_formatting_func else formatting_func(element),\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=max_seq_length,\n",
    "            return_overflowing_tokens=False,\n",
    "            return_length=False,\n",
    "        )\n",
    "\n",
    "        # if use_formatting_func and not self._dataset_sanity_checked:\n",
    "        #     if not isinstance(formatting_func(element), list):\n",
    "        #         raise ValueError(\n",
    "        #             \"The `formatting_func` should return a list of processed strings since it can lead to silent bugs.\"\n",
    "        #         )\n",
    "        #     else:\n",
    "        #         self._dataset_sanity_checked = True\n",
    "\n",
    "        return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n",
    "\n",
    "    signature_columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "\n",
    "    extra_colmuns = list(set(dataset.column_names) - set(signature_columns))\n",
    "\n",
    "    # if not remove_unused_columns and len(extra_colmuns) > 0:\n",
    "    #     warnings.warn(\n",
    "    #         \"You passed `remove_unused_columns=False` on a non-packed dataset. This might create some issues with the default collator and yield to errors. If you want to \"\n",
    "    #         f\"inspect dataset other columns (in this case {extra_colmuns}), you can subclass `DataCollatorForLanguageModeling` in case you used the default collator and create your own data collator in order to inspect the unused dataset columns.\"\n",
    "    #     )\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names if remove_unused_columns else None,\n",
    "        num_proc=1,\n",
    "        batch_size=32,\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = BASE_MISTRAL_TEMPLATE\n",
    "sft_peft.templater = TemplateLLMQuerySender(None, template, start_seq='[', end_seq=']')\n",
    "sft_peft.input_column = \"basic_input\"\n",
    "sft_peft.target_column = \"target_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = _prepare_non_packed_dataloader(tokenizer, datasets['test'], None, None, format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 513\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator(tokenized_dataset, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from sft_peft import main, parse_args\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args([\n",
    "    \"--model\", \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"--train-data\", \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_train.pkl\",\n",
    "    \"--target-column\", \"target_raw\",\n",
    "    \"--input-column\", \"basic_input\",\n",
    "    \"--valid-data\", \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_valid.pkl\",\n",
    "    \"--start-tag\", \"[query]\",\n",
    "    \"--end-tag\", \"[/query]\",\n",
    "    \"--rvalue\", \"32\",\n",
    "    \"--lora-dropout\", \"0\",\n",
    "    \"--batch-size\", \"1\",\n",
    "    \"--gradient-accumulation\", str(4),\n",
    "    \"--packing\", \"0\",\n",
    "    \"--neft-tune-alpha\", \"0\",\n",
    "    \"--epochs\", \"1\",\n",
    "    \"--output\", \".\",\n",
    "    \"--save-name\", \"test_debug\",\n",
    "    \"--run-name\", f\"test_debug\",\n",
    "    \"--random-seed\", \"42\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "    print(list(zip(tokens, token_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁[', 733), ('/', 28748), ('INST', 16289), (']', 28793)]\n",
      "[('▁[', 733), ('/', 28748), ('INST', 16289), (']', 28793), ('▁[', 733), ('query', 3385), (']', 28793)]\n",
      "[('▁[', 733), ('/', 28748), ('INST', 16289), (']', 28793), ('▁[', 733), ('query', 3385), (']', 28793)]\n"
     ]
    }
   ],
   "source": [
    "print_tokens_with_ids(\"[/INST]\")\n",
    "print_tokens_with_ids(\"[/INST] [query]\")\n",
    "print_tokens_with_ids(\"[/INST] [query]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from prompts_template import ELABORATE_INSTRUCTION, BASE_MISTRAL_TEMPLATE\n",
    "from libwikidatallm.TemplateLLMQuerySender import TemplateLLMQuerySender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your assignment involves a two-step process:\n",
      "First, you will receive a task described in a single sentence. This task outlines what information you need to find or what question you need to answer.\n",
      "Then, you will create a SPARQL Query: Based on the task given, you will write a SPARQL query. SPARQL is a specialized query language used to retrieve and manipulate data stored in Resource Description Framework (RDF) format. Your goal is to craft a query that, when executed, fetches the data or answers required by the initial instruction.\n",
      "Make sure your SPARQL query is correctly formulated so that, upon execution, it produces the desired result matching the task's requirements.\n",
      "Answer this following instruction:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>description</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>start_with_SELECT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT ?property ?propertyType ?propertyLabel ...</td>\n",
       "      <td>Wikidata properties in numerical order</td>\n",
       "      <td>Counting stuff on Wikidata\\nAll Wikidata prope...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>267</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT ?id ?idLabel ?idDescription ?new{\\n?id ...</td>\n",
       "      <td>Wikidata properties excluding external IDs</td>\n",
       "      <td>Counting stuff on Wikidata\\nVariation of the a...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>296</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?...</td>\n",
       "      <td></td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of scientifi...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>197</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT (COUNT(DISTINCT ?article) AS ?count)\\nW...</td>\n",
       "      <td>Count of fictional characters</td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of fictional...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>203</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item...</td>\n",
       "      <td>Count of items with coordinate locations</td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of items wit...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>186</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  SELECT ?property ?propertyType ?propertyLabel ...   \n",
       "1  SELECT ?id ?idLabel ?idDescription ?new{\\n?id ...   \n",
       "2  SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?...   \n",
       "3  SELECT (COUNT(DISTINCT ?article) AS ?count)\\nW...   \n",
       "4  SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item...   \n",
       "\n",
       "                                  description  \\\n",
       "0      Wikidata properties in numerical order   \n",
       "1  Wikidata properties excluding external IDs   \n",
       "2                                               \n",
       "3               Count of fictional characters   \n",
       "4    Count of items with coordinate locations   \n",
       "\n",
       "                                             context  \\\n",
       "0  Counting stuff on Wikidata\\nAll Wikidata prope...   \n",
       "1  Counting stuff on Wikidata\\nVariation of the a...   \n",
       "2  Counting stuff on Wikidata\\nCount of scientifi...   \n",
       "3  Counting stuff on Wikidata\\nCount of fictional...   \n",
       "4  Counting stuff on Wikidata\\nCount of items wit...   \n",
       "\n",
       "                                              prompt  num_tokens  \\\n",
       "0  <s>[INST] <<SYS>>This is a conversation betwee...         267   \n",
       "1  <s>[INST] <<SYS>>This is a conversation betwee...         296   \n",
       "2  <s>[INST] <<SYS>>This is a conversation betwee...         197   \n",
       "3  <s>[INST] <<SYS>>This is a conversation betwee...         203   \n",
       "4  <s>[INST] <<SYS>>This is a conversation betwee...         186   \n",
       "\n",
       "   start_with_SELECT  \n",
       "0               True  \n",
       "1               True  \n",
       "2               True  \n",
       "3               True  \n",
       "4               True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"../datasets/final_queries_v1.7.json\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "print(ELABORATE_INSTRUCTION)\n",
    "\n",
    "templater = TemplateLLMQuerySender(llm=None, template_text=BASE_MISTRAL_TEMPLATE, start_seq=\"[\", end_seq=\"]\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_query = dataset['query'].loc[dataset['query'].map(len).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2739"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_prompts(query, templater:TemplateLLMQuerySender, template:str, start_tag:str = '`sparql\\n', end_tag:str = \"`\"):\n",
    "    start_tag = '`sparql\\n'\n",
    "    end_tag = \"`\"\n",
    "\n",
    "    x = templater.apply_template({\n",
    "        \"system_prompt\": template,\n",
    "        \"prompt\": \"\\\"\"+ (\"abigword \" * 20) +\"\\\"\"\n",
    "    })\n",
    "    x += f\"{start_tag}{query}{end_tag}\"\n",
    "    return x\n",
    "\n",
    "longest_prompt = prepare_prompts(longest_query, templater=templater, template=ELABORATE_INSTRUCTION, start_tag=\"`sparql\\n\", end_tag=\"`\")\n",
    "len(tokenizer(longest_prompt)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "import json\n",
    "import random\n",
    "from prompts_template import ELABORATE_INSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = Path(\"../tests/scriptstests/data/compute_metrics_real_data_from_training_1\")\n",
    "epochs_data = [json.loads((folder_path / f\"Mistral-7B-Instruct-v0.2_rv32-ld0-bs1-ga4-gc1-p0-nta0-e3-ctx3072-q4bit-template-template-stsparql_compute_metrics_{i}.json\").read_text()) for i in range(3)]\n",
    "\n",
    "len(epochs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['preds', 'labels', 'decoded_labels', 'decoded_preds'])\n"
     ]
    }
   ],
   "source": [
    "columns = epochs_data[0].keys()\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_answer_and_preds(row_number:int = 0):\n",
    "    print(f\"LABEL n°{row_number}\",\"=\" * 98)\n",
    "    print(epochs_data[0][\"decoded_labels\"][row_number])\n",
    "    print(\"EPOCH 0\",\"=\" * 100)\n",
    "    print(epochs_data[0][\"decoded_preds\"][row_number])\n",
    "    print(\"EPOCH 1\",\"-\" * 100)\n",
    "    print(epochs_data[1][\"decoded_preds\"][row_number])\n",
    "    print(\"EPOCH 2\",\"-\" * 100)\n",
    "    print(epochs_data[2][\"decoded_preds\"][row_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your assignment involves a two-step process:\n",
      "First, you will receive a task described in a single sentence. This task outlines what information you need to find or what question you need to answer.\n",
      "Then, you will create a SPARQL Query: Based on the task given, you will write a SPARQL query. SPARQL is a specialized query language used to retrieve and manipulate data stored in Resource Description Framework (RDF) format. Your goal is to craft a query that, when executed, fetches the data or answers required by the initial instruction.\n",
      "Make sure your SPARQL query is correctly formulated so that, upon execution, it produces the desired result matching the task's requirements.\n",
      "Answer this following instruction:\n"
     ]
    }
   ],
   "source": [
    "print(ELABORATE_INSTRUCTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL n°110 ==================================================================================================\n",
      "`sparql\n",
      "PREFIX prov: <http://www.w3.org/ns/prov#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT (SUBSTR(STR(?item),32) AS ?wdq) (SUBSTR(STR(?prop),32) AS ?wdp) ?object ?url WHERE {\n",
      "?item wdt:[property:collection] wd:[entity:Pitt Rivers Museum].\n",
      "?item wdt:[property:described at URL] ?url FILTER CONTAINS(STR(?url), \"ox.ac.uk\").\n",
      "?item ?p ?statement.\n",
      "?prop wikibase:claim ?p; wikibase:statementProperty ?ps FILTER (?prop != wd:[property:described at URL]) FILTER (?prop != wd:[property:full work available at URL])\n",
      "FILTER (?prop != wd:[property:image]) FILTER (?prop != wd:[property:inception]).\n",
      "?statement ?ps ?object .\n",
      "FILTER NOT EXISTS {?statement prov:wasDerivedFrom ?derivedFrom}\n",
      "} ORDER BY ?item`\n",
      "EPOCH 0 ====================================================================================================\n",
      "Questionader: task is creating lot-part process:\n",
      "\n",
      ", you will need a list to in a text sentence.\n",
      " sentence willlines the you you need to find. what action you need to answer.\n",
      "\n",
      ", you will receive a responseINGQL query to\n",
      " on the task,, write will write a SPARQL query to\n",
      "PARQL queries a query query language for to retrieve and manipulate data stored in R Description Framework (RDF) format.\n",
      " query is to write a query that retriev when executed, willches the data you answers the by the task task.\n",
      "\n",
      " sure to queryPARQL query is as formatted and that it when execution, it returns the desired results. the task.s instruction.\n",
      "\n",
      "swer the task task: FindFind all that the databasei Museum collection aclearenced images.\" which of than 'Pribed at'' 'image name description at URL', 'image available ' 'imageption'.\"\n",
      "`>\n",
      " `sparql\n",
      "PREFIX b: <http://www.w3.org/ns/prov#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      " wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ??STR(STR(?item), 2) AS ?item)) (SUBSTR(STR(?item),32) AS ?wdp) ?item ?object ? {\n",
      "?item wdt:[property:collection] wd:[entity:Pitt Rivers Museum]\n",
      "?item wdt:[property:described at URL] ?url.ILTER NOTTAINS(STR(?url), \"wikfordac.uk\")\n",
      "?item wprop ?object.\n",
      "?statement wikibase:direct ?p.\n",
      "ikibase:propertyProperty ?statement.ILTER NOT?ps != wd:[property:described at URL] FILTER (?prop != wd:[property:full work available at URL]) F? NOT?prop != wdt:[property:image]) FILTER (?prop != wd:[property:inception])\n",
      "?statement provprop ?object.\n",
      "? ( EXISTS { ?statement prov:wasDerivedFrom ?urlivedFrom .\n",
      "FILTER\n",
      "DER BY ?wd`\n",
      "EPOCH 1 ----------------------------------------------------------------------------------------------------\n",
      "Questionpackageer: task is creating lot-part process:\n",
      "\n",
      ", you will need a list to in a text sentence.\n",
      " sentence willlines the you you need to find. what action you need to answer.\n",
      "\n",
      ", you will receive a responseINGQL query to\n",
      " on the task,, write will write a SPARQL query to\n",
      "PARQL queries a query query language for to retrieve and manipulate data on in R Description Framework (RDF) format.\n",
      " query is to write a query that retriev when executed, willches the data you answers the by the task task.\n",
      "\n",
      " sure to queryPARQL query is clear formatted and that it when execution, it returns the desired results. the task.s instruction.\n",
      "\n",
      "swer the task task: FindFind all that the databasei Museum collection aclearenced images.\" their of than 'Pribed at'' 'image view description at URL', 'image available ' 'imageption'.\"\n",
      "`>` `sparql\n",
      "PREFIX b: <http://www.w3.org/ns/prov#>\n",
      "PREFIX prdt: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      " wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ??STR(STR(?item), 0) AS ?item)) ?SUBSTR(STR(?item),32) AS ?wdp) ?item ?object ? {\n",
      "?item wdt:[property:collection] wd:[entity:Pitt Rivers Museum];\n",
      "?item provik:[property:described at URL] ?url.ILTER NOTTAINS(STR(?url), \"wikfordac.uk\")\n",
      "?item ?prop ?object.\n",
      "?prop wikibase:direct ?p.\n",
      "ikibase:propertyProperty ?statement.ILTER (?ps != wd:[entity:described at URL] FILTER (?prop != wd:[property:full work available at URL]) FFILTER NOT?prop != wd:[property:image]) FILTER (?prop != wd:[property:inception])\n",
      "?statement ?prop ?object.\n",
      "? NOT EXISTS { ?prop prov:wasDerivedFrom ?refivedFrom .\n",
      "FILTER ORDER BY ?wd`\n",
      "EPOCH 2 ----------------------------------------------------------------------------------------------------\n",
      "Questionpackageer: task is creating lot-part process:\n",
      "\n",
      ", you will need a list to in a text sentence.\n",
      " sentence willlines what you you need to find. what action you need to answer.\n",
      "\n",
      ", you will receive a responseINGQL query to a on the task,, write will write a SPARQL query to\n",
      "PARQL queries a query query language for to retrieve and manipulate data on in R Description Framework (RDF) format.\n",
      " query is to write a query that retriev when executed, willches the data you answers the by the task task.\n",
      "\n",
      " sure to queryPARQL query is as formatted and that it when execution, it returns the desired results. the task.s instruction.\n",
      "\n",
      "swer the task task: FindFind all that the databasei Museum collection aclearenced images.\" their of than 'Pribed at'' 'image view available at URL', 'image available ' 'imageption'.\"\n",
      "`>` `sparql\n",
      "PREFIX b: <http://www.w3.org/ns/prov#>\n",
      "PREFIX prdt: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      " wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ?SUBSTR(STR(?item), 0) AS ?item)) ?GROUPSTR(STR(?item),32) AS ?wdp) ?item ?value ? {\n",
      "?item wdt:[property:collection] wd:[entity:Pitt Rivers Museum];\n",
      "?item provik:[property:described at URL] ?url.ILTER NOTTAINS(STR(?url), \"wikfordac.uk/\n",
      "?item provprop ?object.\n",
      "?statement wikibase:direct ?p.\n",
      "ikibase:statementProperty ?statement.ILTER (?ps != wd:[entity:described at URL] FILTER (?prop != wd:[property:full work available at URL]) FFILTER NOT?prop != wd:[property:image]) FILTER (?prop != wd:[property:inception])\n",
      "?statement ?prop ?object.\n",
      "? NOT EXISTS { ?prop prov:wasDerivedFrom ?refivedFrom .\n",
      "FILTER ORDER BY ?wd`\n"
     ]
    }
   ],
   "source": [
    "n_rows = len(epochs_data[0][\"decoded_preds\"])\n",
    "\n",
    "print_label_answer_and_preds(random.randint(0, n_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, load_peft_weights\n",
    "from prompts_template import ELABORATE_INSTRUCTION, BASE_MISTRAL_TEMPLATE\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "adapter_path = \"../outputs/batch_run/learning_experiment_long_epochs_4/models/M-7B-I-v0.2_oawtor-rv256-rm1-ld0-bs2-ga8-gc1-p0-nta0-e10-ctx3072-qno-template-template-stsparql_adapters\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecbc7bb265d4233b28cd54e3c0d3207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(model, adapter_path, is_trainable=False)\n",
    "except RuntimeError as e: # RuntimeError => Might be padding token is left in the embedding when saved\n",
    "    d_adpt = load_peft_weights(adapter_path)\n",
    "    len_d_adpt = len(d_adpt.get('base_model.model.lm_head.weight', 0))\n",
    "    if len_d_adpt != 0 and len_d_adpt != len(tokenizer):\n",
    "        model.resize_token_embeddings(len(tokenizer) + 1)\n",
    "        model = PeftModel.from_pretrained(model, adapter_path, is_trainable=False)\n",
    "    else:\n",
    "        raise e\n",
    "except Exception as e:\n",
    "    try:\n",
    "        model = PeftModel.from_pretrained(model, os.path.abspath(adapter_path), is_trainable=False)\n",
    "    except RuntimeError as e: # RuntimeError => Might be padding token is left in the embedding when saved\n",
    "        d_adpt = load_peft_weights(adapter_path)\n",
    "        len_d_adpt = len(d_adpt.get('base_model.model.lm_head.weight', 0))\n",
    "        if len_d_adpt != 0 and len_d_adpt != len(tokenizer):\n",
    "            model.resize_token_embeddings(len(tokenizer) + 1)\n",
    "            model = PeftModel.from_pretrained(model, os.path.abspath(adapter_path), is_trainable=False)\n",
    "        else:\n",
    "            raise e\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32001"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_adpt = load_peft_weights(adapter_path)\n",
    "len(d_adpt.get('base_model.model.lm_head.weight', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model.model.lm_head.weight',\n",
       " 'base_model.model.model.embed_tokens.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(d_adpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32000, 4096)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "modified_tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified_tokenizer.pad_token='[PAD]'\n",
      "modified_tokenizer.pad_token_id=32000\n"
     ]
    }
   ],
   "source": [
    "print(f\"{modified_tokenizer.pad_token=}\")\n",
    "print(f\"{modified_tokenizer.pad_token_id=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from generate_finetune_dataset import keep_working_queries\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>description</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>start_with_SELECT</th>\n",
       "      <th>query_templated</th>\n",
       "      <th>basic_prompt</th>\n",
       "      <th>basic_num_tokens</th>\n",
       "      <th>basic_result</th>\n",
       "      <th>...</th>\n",
       "      <th>basic_is_skipped</th>\n",
       "      <th>basic_is_prompt_too_long</th>\n",
       "      <th>templated_prompt</th>\n",
       "      <th>templated_num_tokens</th>\n",
       "      <th>templated_result</th>\n",
       "      <th>templated_full_answer</th>\n",
       "      <th>templated_is_skipped</th>\n",
       "      <th>templated_is_prompt_too_long</th>\n",
       "      <th>execution</th>\n",
       "      <th>executed_query</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT ?property ?propertyType ?propertyLabel ...</td>\n",
       "      <td>Wikidata properties in numerical order</td>\n",
       "      <td>Counting stuff on Wikidata\\nAll Wikidata prope...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>267</td>\n",
       "      <td>True</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"SELECT ?property ?propertyTyp...</td>\n",
       "      <td>226</td>\n",
       "      <td>[\"Write a SparQL query to retrieve Wikidata pr...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"PREFIX bd: &lt;http://www.bigdat...</td>\n",
       "      <td>289</td>\n",
       "      <td>[\"Write a SparQL query to retrieve all Wikidat...</td>\n",
       "      <td>{'content': ' 1. \"Write a SparQL query to retr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'property': {'type': 'uri', 'value': 'http:/...</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT ?id ?idLabel ?idDescription ?new{\\n?id ...</td>\n",
       "      <td>Wikidata properties excluding external IDs</td>\n",
       "      <td>Counting stuff on Wikidata\\nVariation of the a...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>296</td>\n",
       "      <td>True</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"SELECT ?id ?idLabel ?idDescri...</td>\n",
       "      <td>259</td>\n",
       "      <td>[\"Write a SparQL query to retrieve the ID, lab...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"PREFIX bd: &lt;http://www.bigdat...</td>\n",
       "      <td>322</td>\n",
       "      <td>[\"Write a SparQL query to retrieve the ID, lab...</td>\n",
       "      <td>{'content': ' 1. \"Write a SparQL query to retr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'id': {'type': 'uri', 'value': 'http://www.w...</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?...</td>\n",
       "      <td></td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of scientifi...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>197</td>\n",
       "      <td>True</td>\n",
       "      <td>PREFIX wd: &lt;http://www.wikidata.org/entity/&gt;\\n...</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"SELECT (COUNT(?article) AS ?c...</td>\n",
       "      <td>159</td>\n",
       "      <td>[\"Find the total number of scientific articles...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"PREFIX wd: &lt;http://www.wikida...</td>\n",
       "      <td>201</td>\n",
       "      <td>[\"Write a SparQL query to retrieve the count o...</td>\n",
       "      <td>{'content': ' 1. \"Write a SparQL query to retr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>timeout</td>\n",
       "      <td>PREFIX wd: &lt;http://www.wikidata.org/entity/&gt;\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT (COUNT(DISTINCT ?article) AS ?count)\\nW...</td>\n",
       "      <td>Count of fictional characters</td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of fictional...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>203</td>\n",
       "      <td>True</td>\n",
       "      <td>PREFIX wd: &lt;http://www.wikidata.org/entity/&gt;\\n...</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"SELECT (COUNT(DISTINCT ?artic...</td>\n",
       "      <td>165</td>\n",
       "      <td>[\"Find the number of distinct fictional charac...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"PREFIX wd: &lt;http://www.wikida...</td>\n",
       "      <td>210</td>\n",
       "      <td>[\"Write a query to find the total number of di...</td>\n",
       "      <td>{'content': ' 1. \"Write a query to find the to...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'count': {'datatype': 'http://www.w3.org/200...</td>\n",
       "      <td>PREFIX wd: &lt;http://www.wikidata.org/entity/&gt;\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item...</td>\n",
       "      <td>Count of items with coordinate locations</td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of items wit...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>186</td>\n",
       "      <td>True</td>\n",
       "      <td>PREFIX wdt: &lt;http://www.wikidata.org/prop/dire...</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"SELECT (COUNT(?item) AS ?coun...</td>\n",
       "      <td>148</td>\n",
       "      <td>[\"How many items in Wikidata have recorded coo...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"PREFIX wdt: &lt;http://www.wikid...</td>\n",
       "      <td>172</td>\n",
       "      <td>[\"Write a SparQL query to retrieve the number ...</td>\n",
       "      <td>{'content': ' 1. \"Write a SparQL query to retr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'count': {'datatype': 'http://www.w3.org/200...</td>\n",
       "      <td>PREFIX wdt: &lt;http://www.wikidata.org/prop/dire...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   query  \\\n",
       "index                                                      \n",
       "0      SELECT ?property ?propertyType ?propertyLabel ...   \n",
       "1      SELECT ?id ?idLabel ?idDescription ?new{\\n?id ...   \n",
       "2      SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?...   \n",
       "3      SELECT (COUNT(DISTINCT ?article) AS ?count)\\nW...   \n",
       "4      SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item...   \n",
       "\n",
       "                                      description  \\\n",
       "index                                               \n",
       "0          Wikidata properties in numerical order   \n",
       "1      Wikidata properties excluding external IDs   \n",
       "2                                                   \n",
       "3                   Count of fictional characters   \n",
       "4        Count of items with coordinate locations   \n",
       "\n",
       "                                                 context  \\\n",
       "index                                                      \n",
       "0      Counting stuff on Wikidata\\nAll Wikidata prope...   \n",
       "1      Counting stuff on Wikidata\\nVariation of the a...   \n",
       "2      Counting stuff on Wikidata\\nCount of scientifi...   \n",
       "3      Counting stuff on Wikidata\\nCount of fictional...   \n",
       "4      Counting stuff on Wikidata\\nCount of items wit...   \n",
       "\n",
       "                                                  prompt  num_tokens  \\\n",
       "index                                                                  \n",
       "0      <s>[INST] <<SYS>>This is a conversation betwee...         267   \n",
       "1      <s>[INST] <<SYS>>This is a conversation betwee...         296   \n",
       "2      <s>[INST] <<SYS>>This is a conversation betwee...         197   \n",
       "3      <s>[INST] <<SYS>>This is a conversation betwee...         203   \n",
       "4      <s>[INST] <<SYS>>This is a conversation betwee...         186   \n",
       "\n",
       "       start_with_SELECT                                    query_templated  \\\n",
       "index                                                                         \n",
       "0                   True  PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...   \n",
       "1                   True  PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...   \n",
       "2                   True  PREFIX wd: <http://www.wikidata.org/entity/>\\n...   \n",
       "3                   True  PREFIX wd: <http://www.wikidata.org/entity/>\\n...   \n",
       "4                   True  PREFIX wdt: <http://www.wikidata.org/prop/dire...   \n",
       "\n",
       "                                            basic_prompt  basic_num_tokens  \\\n",
       "index                                                                        \n",
       "0      <s>[INST] QUERY=\"SELECT ?property ?propertyTyp...               226   \n",
       "1      <s>[INST] QUERY=\"SELECT ?id ?idLabel ?idDescri...               259   \n",
       "2      <s>[INST] QUERY=\"SELECT (COUNT(?article) AS ?c...               159   \n",
       "3      <s>[INST] QUERY=\"SELECT (COUNT(DISTINCT ?artic...               165   \n",
       "4      <s>[INST] QUERY=\"SELECT (COUNT(?item) AS ?coun...               148   \n",
       "\n",
       "                                            basic_result  ...  \\\n",
       "index                                                     ...   \n",
       "0      [\"Write a SparQL query to retrieve Wikidata pr...  ...   \n",
       "1      [\"Write a SparQL query to retrieve the ID, lab...  ...   \n",
       "2      [\"Find the total number of scientific articles...  ...   \n",
       "3      [\"Find the number of distinct fictional charac...  ...   \n",
       "4      [\"How many items in Wikidata have recorded coo...  ...   \n",
       "\n",
       "      basic_is_skipped  basic_is_prompt_too_long  \\\n",
       "index                                              \n",
       "0                False                     False   \n",
       "1                False                     False   \n",
       "2                False                     False   \n",
       "3                False                     False   \n",
       "4                False                     False   \n",
       "\n",
       "                                        templated_prompt templated_num_tokens  \\\n",
       "index                                                                           \n",
       "0      <s>[INST] QUERY=\"PREFIX bd: <http://www.bigdat...                  289   \n",
       "1      <s>[INST] QUERY=\"PREFIX bd: <http://www.bigdat...                  322   \n",
       "2      <s>[INST] QUERY=\"PREFIX wd: <http://www.wikida...                  201   \n",
       "3      <s>[INST] QUERY=\"PREFIX wd: <http://www.wikida...                  210   \n",
       "4      <s>[INST] QUERY=\"PREFIX wdt: <http://www.wikid...                  172   \n",
       "\n",
       "                                        templated_result  \\\n",
       "index                                                      \n",
       "0      [\"Write a SparQL query to retrieve all Wikidat...   \n",
       "1      [\"Write a SparQL query to retrieve the ID, lab...   \n",
       "2      [\"Write a SparQL query to retrieve the count o...   \n",
       "3      [\"Write a query to find the total number of di...   \n",
       "4      [\"Write a SparQL query to retrieve the number ...   \n",
       "\n",
       "                                   templated_full_answer templated_is_skipped  \\\n",
       "index                                                                           \n",
       "0      {'content': ' 1. \"Write a SparQL query to retr...                False   \n",
       "1      {'content': ' 1. \"Write a SparQL query to retr...                False   \n",
       "2      {'content': ' 1. \"Write a SparQL query to retr...                False   \n",
       "3      {'content': ' 1. \"Write a query to find the to...                False   \n",
       "4      {'content': ' 1. \"Write a SparQL query to retr...                False   \n",
       "\n",
       "       templated_is_prompt_too_long  \\\n",
       "index                                 \n",
       "0                             False   \n",
       "1                             False   \n",
       "2                             False   \n",
       "3                             False   \n",
       "4                             False   \n",
       "\n",
       "                                               execution  \\\n",
       "index                                                      \n",
       "0      [{'property': {'type': 'uri', 'value': 'http:/...   \n",
       "1      [{'id': {'type': 'uri', 'value': 'http://www.w...   \n",
       "2                                                timeout   \n",
       "3      [{'count': {'datatype': 'http://www.w3.org/200...   \n",
       "4      [{'count': {'datatype': 'http://www.w3.org/200...   \n",
       "\n",
       "                                          executed_query  \n",
       "index                                                     \n",
       "0      PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...  \n",
       "1      PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...  \n",
       "2      PREFIX wd: <http://www.wikidata.org/entity/>\\n...  \n",
       "3      PREFIX wd: <http://www.wikidata.org/entity/>\\n...  \n",
       "4      PREFIX wdt: <http://www.wikidata.org/prop/dire...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(\"../outputs/dataset_pipeline/fq17_3/fq17_3-generated_prompt-executed.parquet.gzip\")\n",
    "if not path.exists():\n",
    "    raise FileExistsError(f\"No file was found at this path: {path}\")\n",
    "\n",
    "dataset = load_dataset(path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'description', 'context', 'prompt', 'num_tokens',\n",
       "       'start_with_SELECT', 'query_templated', 'basic_prompt',\n",
       "       'basic_num_tokens', 'basic_result', 'basic_full_answer',\n",
       "       'basic_is_skipped', 'basic_is_prompt_too_long', 'templated_prompt',\n",
       "       'templated_num_tokens', 'templated_result', 'templated_full_answer',\n",
       "       'templated_is_skipped', 'templated_is_prompt_too_long', 'execution',\n",
       "       'executed_query'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>templated_result</th>\n",
       "      <th>query_templated</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"Write a SparQL query to retrieve all Wikidat...</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"Write a SparQL query to retrieve the ID, lab...</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"Write a query to find the total number of di...</td>\n",
       "      <td>PREFIX wd: &lt;http://www.wikidata.org/entity/&gt;\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"Write a SparQL query to retrieve the number ...</td>\n",
       "      <td>PREFIX wdt: &lt;http://www.wikidata.org/prop/dire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[\"Write a SparQL query to retrieve the number ...</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        templated_result  \\\n",
       "index                                                      \n",
       "0      [\"Write a SparQL query to retrieve all Wikidat...   \n",
       "1      [\"Write a SparQL query to retrieve the ID, lab...   \n",
       "3      [\"Write a query to find the total number of di...   \n",
       "4      [\"Write a SparQL query to retrieve the number ...   \n",
       "5      [\"Write a SparQL query to retrieve the number ...   \n",
       "\n",
       "                                         query_templated  \n",
       "index                                                     \n",
       "0      PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...  \n",
       "1      PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...  \n",
       "3      PREFIX wd: <http://www.wikidata.org/entity/>\\n...  \n",
       "4      PREFIX wdt: <http://www.wikidata.org/prop/dire...  \n",
       "5      PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped_dataset = keep_working_queries(dataset)\n",
    "\n",
    "columns_to_take = [\n",
    "    \"templated_result\",\n",
    "    \"query_templated\",\n",
    "]\n",
    "\n",
    "dataset_for_tokenization = dropped_dataset[columns_to_take]\n",
    "dataset_for_tokenization.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_concat_dataset)=2294959\n"
     ]
    }
   ],
   "source": [
    "all_concat_dataset = dataset_for_tokenization.apply(lambda x: x['templated_result'][0].strip() + x[\"query_templated\"].strip(), axis=1).sum()\n",
    "print(f\"{len(all_concat_dataset)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(all_concat_dataset)\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.tokens())=824341\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(tokens.tokens())=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry n°353\n",
      "----- Prompts From Original -----\n",
      "\"Write a SPARQL query to retrieve all books with a title, illustrator, publisher, and publication date, but allow for the possibility that some books may not have an illustrator or publisher listed.\"\n",
      "\"Create a SPARQL query to find all books with a title, optional illustrator and publisher information, and a publication date.\"\n",
      "\"Generate a SPARQL query to retrieve books with required title, publication date, and optional illustrator and publisher information.\"\n",
      "----- Original Query -----\n",
      "SELECT ?book ?title ?illustratorLabel ?publisherLabel ?published\n",
      "WHERE\n",
      "{\n",
      "?book wdt:P50 wd:Q35610. # required wdt:P50\n",
      "OPTIONAL { # match all or none from group:\n",
      "?book wdt:P1476 ?title; # required wdt:P1476\n",
      "wdt:P110 ?illustrator; # required wdt:P110\n",
      "wdt:P123 ?publisher; # required wdt:P123\n",
      "wdt:P577 ?published. # required wdt:P577\n",
      "}\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "} \n",
      "\n",
      "----- Prompts From Templated -----\n",
      "\"Write a SPARQL query to retrieve books authored by Arthur Conan Doyle, along with their titles, illustrators, publishers, and publication dates if available.\"\n",
      "\"Create a SPARQL query that returns all books with an author of 'Arthur Conan Doyle', as well as their optional title, illustrator, publisher, and publication date information.\"\n",
      "----- Templated Query -----\n",
      "PREFIX bd: <http://www.bigdata.com/rdf#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ?book ?title ?illustratorLabel ?publisherLabel ?published\n",
      "WHERE\n",
      "{\n",
      "?book wdt:[property:author] wd:[entity:Arthur Conan Doyle]. # required wdt:[property:author]\n",
      "OPTIONAL { # match all or none from group:\n",
      "?book wdt:[property:title] ?title; # required wdt:[property:title]\n",
      "wdt:[property:illustrator] ?illustrator; # required wdt:[property:illustrator]\n",
      "wdt:[property:publisher] ?publisher; # required wdt:[property:publisher]\n",
      "wdt:[property:publication date] ?published. # required wdt:[property:publication date]\n",
      "}\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "entry_iloc_id = random.randint(0, len(dropped_dataset) - 1)\n",
    "entry = dropped_dataset.iloc[entry_iloc_id]\n",
    "print(f\"Entry n°{entry_iloc_id}\")\n",
    "print(\"----- Prompts From Original -----\")\n",
    "print(\"\\n\".join(entry[\"basic_result\"]))\n",
    "print(\"----- Original Query -----\")\n",
    "print(entry[\"query\"], \"\\n\")\n",
    "print(\"----- Prompts From Templated -----\")\n",
    "print(\"\\n\".join(entry[\"templated_result\"]))\n",
    "print(\"----- Templated Query -----\")\n",
    "print(entry[\"query_templated\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
