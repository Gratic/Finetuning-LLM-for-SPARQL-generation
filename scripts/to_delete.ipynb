{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "import pandas as pd\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/batch_run/faith_template5/execution/Mistral-7B-Instruct-v0.2_rv16-ld0.05-bs1-p0-nta1-e3-template_engpeft-t0.2-topp0.95_executed.parquet.gzip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0].translated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0].linked_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0].target_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "print(f\"{tokenizer.unk_token=}\")\n",
    "print(f\"{tokenizer.pad_token=}\")\n",
    "print(f\"{tokenizer.unk_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "print(f\"{tokenizer.unk_token=}\")\n",
    "print(f\"{tokenizer.pad_token=}\")\n",
    "print(f\"{tokenizer.unk_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/batch_run/democracy_template8/execution/Mistral-7B-Instruct-v0.2_rv32-ld0.05-bs1-p0-nta1-e3-template_engpeft-t0.2-topp0.95_executed.parquet.gzip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handpicked_data = df.loc[(~df['execution'].str.startswith(\"exception:\")) & (~df['execution'].isnull()) & (df['execution'].map(len) > 0)].iloc[[1, 2, 3, 206, 207]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tokenizer.batch_encode_plus([f\"`sparql\\n{i}`\" for i in handpicked_data['translated_prompt'].to_list()], add_special_tokens=False, padding='longest', return_tensors='np')['input_ids'].tolist()\n",
    "labels = tokenizer.batch_encode_plus([f\"`sparql\\n{i}`\" for i in handpicked_data['target_template'].to_list()], add_special_tokens=False, padding='longest', return_tensors='np')['input_ids'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data = {\n",
    "    \"preds\": preds,\n",
    "    \"labels\": labels,\n",
    "}\n",
    "\n",
    "path = Path(\"sft_peft_compute_metrics_execute_ok.json\")\n",
    "path.write_text(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "\n",
    "\n",
    "df = load_dataset(\"../outputs/batch_run/lezgo/execution/Mistral-7B-Instruct-v0.2_rv16-ld0-bs1-p0-nta0-e3-template_engpeft-t0.2-topp0.95_executed.parquet.gzip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "df_out = df.head()\n",
    "\n",
    "path = Path(\"evaluation_test.json\")\n",
    "path.write_text(df_out.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_gold = json.loads(Path(\"../outputs/batch_run/lezgo/preprocessed_gold.json\").read_text())\n",
    "df_gold = pd.read_json(data_gold['df_gold_eval'])\n",
    "df_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold_out = df_gold.head()\n",
    "\n",
    "path = Path(\"evaluation_test_gold.json\")\n",
    "path.write_text(df_gold_out.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"../datasets/final_queries_v1.7.json\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = dataset[2549:2553]\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out_v2 = df_out.reset_index(drop=True)\n",
    "df_out_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"templatize_test.json\")\n",
    "path.write_text(df_out_v2.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/dataset_pipeline/fq17_3/fq17_3-split_test.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = df.iloc[0]\n",
    "for c in df.columns:\n",
    "    print(c)\n",
    "    print(entry[c])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/dataset_pipeline/fq17_3/fq17_3-split_test.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2].target_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments, AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sft_peft import format_prompt\n",
    "import sft_peft\n",
    "from prompts_template import BASE_MISTRAL_TEMPLATE, BASE_BASIC_INSTRUCTION\n",
    "from libwikidatallm.TemplateLLMQuerySender import TemplateLLMQuerySender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=\"[/INST]\", tokenizer=tokenizer)\n",
    "datasets = load_dataset(\"pandas\", data_files=\n",
    "                          {\n",
    "                              \"valid\": \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_valid.pkl\",\n",
    "                              \"train\": \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_train.pkl\",\n",
    "                              \"test\": \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_test.pkl\",\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    valid: Dataset({\n",
       "        features: ['basic_input', 'templated_input', 'target_raw', 'target_template', 'index'],\n",
       "        num_rows: 129\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['basic_input', 'templated_input', 'target_raw', 'target_template', 'index'],\n",
       "        num_rows: 1923\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['basic_input', 'templated_input', 'target_raw', 'target_template', 'index'],\n",
       "        num_rows: 513\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from SFTTrainer(Trainer) _prepare_non_packed_dataloader method\n",
    "def _prepare_non_packed_dataloader(\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    dataset_text_field,\n",
    "    max_seq_length,\n",
    "    formatting_func=None,\n",
    "    add_special_tokens=True,\n",
    "    remove_unused_columns=True,\n",
    "):\n",
    "    use_formatting_func = formatting_func is not None and dataset_text_field is None\n",
    "    # self._dataset_sanity_checked = False\n",
    "\n",
    "    # Inspired from: https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt\n",
    "    def tokenize(element):\n",
    "        outputs = tokenizer(\n",
    "            element[dataset_text_field] if not use_formatting_func else formatting_func(element),\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=max_seq_length,\n",
    "            return_overflowing_tokens=False,\n",
    "            return_length=False,\n",
    "        )\n",
    "\n",
    "        # if use_formatting_func and not self._dataset_sanity_checked:\n",
    "        #     if not isinstance(formatting_func(element), list):\n",
    "        #         raise ValueError(\n",
    "        #             \"The `formatting_func` should return a list of processed strings since it can lead to silent bugs.\"\n",
    "        #         )\n",
    "        #     else:\n",
    "        #         self._dataset_sanity_checked = True\n",
    "\n",
    "        return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n",
    "\n",
    "    signature_columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "\n",
    "    extra_colmuns = list(set(dataset.column_names) - set(signature_columns))\n",
    "\n",
    "    # if not remove_unused_columns and len(extra_colmuns) > 0:\n",
    "    #     warnings.warn(\n",
    "    #         \"You passed `remove_unused_columns=False` on a non-packed dataset. This might create some issues with the default collator and yield to errors. If you want to \"\n",
    "    #         f\"inspect dataset other columns (in this case {extra_colmuns}), you can subclass `DataCollatorForLanguageModeling` in case you used the default collator and create your own data collator in order to inspect the unused dataset columns.\"\n",
    "    #     )\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names if remove_unused_columns else None,\n",
    "        num_proc=1,\n",
    "        batch_size=32,\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = BASE_MISTRAL_TEMPLATE\n",
    "sft_peft.templater = TemplateLLMQuerySender(None, template, start_seq='[', end_seq=']')\n",
    "sft_peft.input_column = \"basic_input\"\n",
    "sft_peft.target_column = \"target_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = _prepare_non_packed_dataloader(tokenizer, datasets['test'], None, None, format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 513\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator(tokenized_dataset, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from sft_peft import main, parse_args\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args([\n",
    "    \"--model\", \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"--train-data\", \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_train.pkl\",\n",
    "    \"--target-column\", \"target_raw\",\n",
    "    \"--input-column\", \"basic_input\",\n",
    "    \"--valid-data\", \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_valid.pkl\",\n",
    "    \"--start-tag\", \"[query]\",\n",
    "    \"--end-tag\", \"[/query]\",\n",
    "    \"--rvalue\", \"32\",\n",
    "    \"--lora-dropout\", \"0\",\n",
    "    \"--batch-size\", \"1\",\n",
    "    \"--gradient-accumulation\", str(4),\n",
    "    \"--packing\", \"0\",\n",
    "    \"--neft-tune-alpha\", \"0\",\n",
    "    \"--epochs\", \"1\",\n",
    "    \"--output\", \".\",\n",
    "    \"--save-name\", \"test_debug\",\n",
    "    \"--run-name\", f\"test_debug\",\n",
    "    \"--random-seed\", \"42\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "    print(list(zip(tokens, token_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁[', 733), ('/', 28748), ('INST', 16289), (']', 28793)]\n",
      "[('▁[', 733), ('/', 28748), ('INST', 16289), (']', 28793), ('▁[', 733), ('query', 3385), (']', 28793)]\n",
      "[('▁[', 733), ('/', 28748), ('INST', 16289), (']', 28793), ('▁[', 733), ('query', 3385), (']', 28793)]\n"
     ]
    }
   ],
   "source": [
    "print_tokens_with_ids(\"[/INST]\")\n",
    "print_tokens_with_ids(\"[/INST] [query]\")\n",
    "print_tokens_with_ids(\"[/INST] [query]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from prompts_template import ELABORATE_INSTRUCTION, BASE_MISTRAL_TEMPLATE\n",
    "from libwikidatallm.TemplateLLMQuerySender import TemplateLLMQuerySender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your assignment involves a two-step process:\n",
      "First, you will receive a task described in a single sentence. This task outlines what information you need to find or what question you need to answer.\n",
      "Then, you will create a SPARQL Query: Based on the task given, you will write a SPARQL query. SPARQL is a specialized query language used to retrieve and manipulate data stored in Resource Description Framework (RDF) format. Your goal is to craft a query that, when executed, fetches the data or answers required by the initial instruction.\n",
      "Make sure your SPARQL query is correctly formulated so that, upon execution, it produces the desired result matching the task's requirements.\n",
      "Answer this following instruction:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>description</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>start_with_SELECT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT ?property ?propertyType ?propertyLabel ...</td>\n",
       "      <td>Wikidata properties in numerical order</td>\n",
       "      <td>Counting stuff on Wikidata\\nAll Wikidata prope...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>267</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT ?id ?idLabel ?idDescription ?new{\\n?id ...</td>\n",
       "      <td>Wikidata properties excluding external IDs</td>\n",
       "      <td>Counting stuff on Wikidata\\nVariation of the a...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>296</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?...</td>\n",
       "      <td></td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of scientifi...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>197</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT (COUNT(DISTINCT ?article) AS ?count)\\nW...</td>\n",
       "      <td>Count of fictional characters</td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of fictional...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>203</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item...</td>\n",
       "      <td>Count of items with coordinate locations</td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of items wit...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>186</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  SELECT ?property ?propertyType ?propertyLabel ...   \n",
       "1  SELECT ?id ?idLabel ?idDescription ?new{\\n?id ...   \n",
       "2  SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?...   \n",
       "3  SELECT (COUNT(DISTINCT ?article) AS ?count)\\nW...   \n",
       "4  SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item...   \n",
       "\n",
       "                                  description  \\\n",
       "0      Wikidata properties in numerical order   \n",
       "1  Wikidata properties excluding external IDs   \n",
       "2                                               \n",
       "3               Count of fictional characters   \n",
       "4    Count of items with coordinate locations   \n",
       "\n",
       "                                             context  \\\n",
       "0  Counting stuff on Wikidata\\nAll Wikidata prope...   \n",
       "1  Counting stuff on Wikidata\\nVariation of the a...   \n",
       "2  Counting stuff on Wikidata\\nCount of scientifi...   \n",
       "3  Counting stuff on Wikidata\\nCount of fictional...   \n",
       "4  Counting stuff on Wikidata\\nCount of items wit...   \n",
       "\n",
       "                                              prompt  num_tokens  \\\n",
       "0  <s>[INST] <<SYS>>This is a conversation betwee...         267   \n",
       "1  <s>[INST] <<SYS>>This is a conversation betwee...         296   \n",
       "2  <s>[INST] <<SYS>>This is a conversation betwee...         197   \n",
       "3  <s>[INST] <<SYS>>This is a conversation betwee...         203   \n",
       "4  <s>[INST] <<SYS>>This is a conversation betwee...         186   \n",
       "\n",
       "   start_with_SELECT  \n",
       "0               True  \n",
       "1               True  \n",
       "2               True  \n",
       "3               True  \n",
       "4               True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"../datasets/final_queries_v1.7.json\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "print(ELABORATE_INSTRUCTION)\n",
    "\n",
    "templater = TemplateLLMQuerySender(llm=None, template_text=BASE_MISTRAL_TEMPLATE, start_seq=\"[\", end_seq=\"]\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_query = dataset['query'].loc[dataset['query'].map(len).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2739"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_prompts(query, templater:TemplateLLMQuerySender, template:str, start_tag:str = '`sparql\\n', end_tag:str = \"`\"):\n",
    "    start_tag = '`sparql\\n'\n",
    "    end_tag = \"`\"\n",
    "\n",
    "    x = templater.apply_template({\n",
    "        \"system_prompt\": template,\n",
    "        \"prompt\": \"\\\"\"+ (\"abigword \" * 20) +\"\\\"\"\n",
    "    })\n",
    "    x += f\"{start_tag}{query}{end_tag}\"\n",
    "    return x\n",
    "\n",
    "longest_prompt = prepare_prompts(longest_query, templater=templater, template=ELABORATE_INSTRUCTION, start_tag=\"`sparql\\n\", end_tag=\"`\")\n",
    "len(tokenizer(longest_prompt)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "import json\n",
    "import random\n",
    "from prompts_template import ELABORATE_INSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = Path(\"../tests/scriptstests/data/compute_metrics_real_data_from_training_1\")\n",
    "epochs_data = [json.loads((folder_path / f\"Mistral-7B-Instruct-v0.2_rv32-ld0-bs1-ga4-gc1-p0-nta0-e3-ctx3072-q4bit-template-template-stsparql_compute_metrics_{i}.json\").read_text()) for i in range(3)]\n",
    "\n",
    "len(epochs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['preds', 'labels', 'decoded_labels', 'decoded_preds'])\n"
     ]
    }
   ],
   "source": [
    "columns = epochs_data[0].keys()\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_answer_and_preds(row_number:int = 0):\n",
    "    print(f\"LABEL n°{row_number}\",\"=\" * 98)\n",
    "    print(epochs_data[0][\"decoded_labels\"][row_number])\n",
    "    print(\"EPOCH 0\",\"=\" * 100)\n",
    "    print(epochs_data[0][\"decoded_preds\"][row_number])\n",
    "    print(\"EPOCH 1\",\"-\" * 100)\n",
    "    print(epochs_data[1][\"decoded_preds\"][row_number])\n",
    "    print(\"EPOCH 2\",\"-\" * 100)\n",
    "    print(epochs_data[2][\"decoded_preds\"][row_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your assignment involves a two-step process:\n",
      "First, you will receive a task described in a single sentence. This task outlines what information you need to find or what question you need to answer.\n",
      "Then, you will create a SPARQL Query: Based on the task given, you will write a SPARQL query. SPARQL is a specialized query language used to retrieve and manipulate data stored in Resource Description Framework (RDF) format. Your goal is to craft a query that, when executed, fetches the data or answers required by the initial instruction.\n",
      "Make sure your SPARQL query is correctly formulated so that, upon execution, it produces the desired result matching the task's requirements.\n",
      "Answer this following instruction:\n"
     ]
    }
   ],
   "source": [
    "print(ELABORATE_INSTRUCTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL n°73 ==================================================================================================\n",
      "`sparql\n",
      "PREFIX bd: <http://www.bigdata.com/rdf#>\n",
      "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT DISTINCT ?a ?aLabel ?b ?bLabel\n",
      "{?a wdt:[property:instance of] wd:[entity:human];\n",
      "wdt:[property:date of birth] ?birth;\n",
      "wdt:[property:influenced by] ?b .\n",
      "?b wdt:[property:instance of] wd:[entity:human] .\n",
      "FILTER (?birth > \"1590-01-01\"^^xsd:dateTime && ?birth < \"1750-01-01\"^^xsd:dateTime)\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "}`\n",
      "EPOCH 0 ====================================================================================================\n",
      "Questionader: task is creating lot-part process:\n",
      "\n",
      ", you will need a list to in a text sentence.\n",
      " sentence willlines the you you need to find. what action you need to answer.\n",
      "\n",
      ", you will receive a responseINGQL query to\n",
      " on the task,, write will write a SPARQL query to\n",
      "PARQL queries a query query language for to retrieve and manipulate data stored in R Description Framework (RDF) format.\n",
      " query is to write a query that retriev when executed, willches the data you answers the by the task task.\n",
      "\n",
      " sure to queryPARQL query is as formatted and that it when execution, it returns the desired results. the task.s instruction.\n",
      "\n",
      "swer the task task: FindFind all who in 1900 and 1600.\" and with their birth and.\" places place they are.\"\n",
      "``\n",
      " `sparql\n",
      "PREFIX bd: <http://www.bigdata.com/rdf#>\n",
      "PREFIX wsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ?ISTINCT ?human ?aLabel ?birthbLabel ?W\n",
      "a wdt:[property:instance of] wd:[entity:human]\n",
      "wdt:[property:date of birth] ?b..\n",
      "wdt:[property:influenced by] ?b.\n",
      "FILTERbirthdt:[property:instance of] wd:[entity:human] .\n",
      "FILTER(?birth >= \"1590-01-01T^^xsd:dateTime && ?birth < \"1750-01-01\"^^xsd:dateTime)\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "}`\n",
      "EPOCH 1 ----------------------------------------------------------------------------------------------------\n",
      "Questionpackageer: task is creating lot-part process:\n",
      "\n",
      ", you will need a list to in a text sentence.\n",
      " sentence willlines the you you need to find. what action you need to answer.\n",
      "\n",
      ", you will receive a responseINGQL query to\n",
      " on the task,, write will write a SPARQL query to\n",
      "PARQL queries a query query language for to retrieve and manipulate data on in R Description Framework (RDF) format.\n",
      " query is to write a query that retriev when executed, willches the data you answers the by the task task.\n",
      "\n",
      " sure to queryPARQL query is clear formatted and that it when execution, it returns the desired results. the task.s instruction.\n",
      "\n",
      "swer the task task: FindFind all who in 1900 and 1600.\" and with their birth and.\" places places they have.\"\n",
      "``\n",
      " `sparql\n",
      "PREFIX bd: <http://www.bigdata.com/rdf#>\n",
      "PREFIX wsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ?ISTINCT ?human ?aLabel ?birthbLabel ?W\n",
      "a wdt:[property:instance of] wd:[entity:human]\n",
      "wdt:[property:date of birth] ?b..\n",
      "wdt:[property:influenced by] ?b.\n",
      "FILTERb wdt:[property:instance of] wd:[entity:human]. .\n",
      "FILTER(?birth >= \"1590-01-01T^^xsd:dateTime && ?birth < \"1750-01-01\"^^xsd:dateTime)\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "}`\n",
      "EPOCH 2 ----------------------------------------------------------------------------------------------------\n",
      "Questionpackageer: task is creating lot-part process:\n",
      "\n",
      ", you will need a list to in a text sentence.\n",
      " sentence willlines what you you need to find. what action you need to answer.\n",
      "\n",
      ", you will receive a responseINGQL query to a on the task,, write will write a SPARQL query to\n",
      "PARQL queries a query query language for to retrieve and manipulate data on in R Description Framework (RDF) format.\n",
      " query is to write a query that retriev when executed, willches the data you answers the by the task task.\n",
      "\n",
      " sure to queryPARQL query is as formatted and that it when execution, it returns the desired results. the task.s instruction.\n",
      "\n",
      "swer the task task: FindFind all who in 1900 and 1600.\" and with their birth and.\" places places they have.\"\n",
      "``\n",
      " `sparql\n",
      "PREFIX bd: <http://www.bigdata.com/rdf#>\n",
      "PREFIX wsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ?ISTINCT ?human ?aLabel ?birthbLabel ?W\n",
      "a wdt:[property:instance of] wd:[entity:human]\n",
      "wdt:[property:date of birth] ?b..\n",
      "wdt:[property:influenced by] ?b.\n",
      "FILTERb wdt:[property:instance of] wd:[entity:human] .\n",
      "FILTER(?birth >= \"1590-01-01T^^xsd:dateTime && ?birth < \"1750-01-01\"^^xsd:dateTime)\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "}`\n"
     ]
    }
   ],
   "source": [
    "n_rows = len(epochs_data[0][\"decoded_preds\"])\n",
    "\n",
    "print_label_answer_and_preds(random.randint(0, n_rows))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
