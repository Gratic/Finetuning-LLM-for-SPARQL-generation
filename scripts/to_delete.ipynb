{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "import pandas as pd\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/batch_run/faith_template5/execution/Mistral-7B-Instruct-v0.2_rv16-ld0.05-bs1-p0-nta1-e3-template_engpeft-t0.2-topp0.95_executed.parquet.gzip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0].translated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0].linked_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0].target_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "print(f\"{tokenizer.unk_token=}\")\n",
    "print(f\"{tokenizer.pad_token=}\")\n",
    "print(f\"{tokenizer.unk_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "print(f\"{tokenizer.unk_token=}\")\n",
    "print(f\"{tokenizer.pad_token=}\")\n",
    "print(f\"{tokenizer.unk_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/batch_run/democracy_template8/execution/Mistral-7B-Instruct-v0.2_rv32-ld0.05-bs1-p0-nta1-e3-template_engpeft-t0.2-topp0.95_executed.parquet.gzip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handpicked_data = df.loc[(~df['execution'].str.startswith(\"exception:\")) & (~df['execution'].isnull()) & (df['execution'].map(len) > 0)].iloc[[1, 2, 3, 206, 207]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tokenizer.batch_encode_plus([f\"`sparql\\n{i}`\" for i in handpicked_data['translated_prompt'].to_list()], add_special_tokens=False, padding='longest', return_tensors='np')['input_ids'].tolist()\n",
    "labels = tokenizer.batch_encode_plus([f\"`sparql\\n{i}`\" for i in handpicked_data['target_template'].to_list()], add_special_tokens=False, padding='longest', return_tensors='np')['input_ids'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data = {\n",
    "    \"preds\": preds,\n",
    "    \"labels\": labels,\n",
    "}\n",
    "\n",
    "path = Path(\"sft_peft_compute_metrics_execute_ok.json\")\n",
    "path.write_text(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "\n",
    "\n",
    "df = load_dataset(\"../outputs/batch_run/lezgo/execution/Mistral-7B-Instruct-v0.2_rv16-ld0-bs1-p0-nta0-e3-template_engpeft-t0.2-topp0.95_executed.parquet.gzip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "df_out = df.head()\n",
    "\n",
    "path = Path(\"evaluation_test.json\")\n",
    "path.write_text(df_out.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_gold = json.loads(Path(\"../outputs/batch_run/lezgo/preprocessed_gold.json\").read_text())\n",
    "df_gold = pd.read_json(data_gold['df_gold_eval'])\n",
    "df_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold_out = df_gold.head()\n",
    "\n",
    "path = Path(\"evaluation_test_gold.json\")\n",
    "path.write_text(df_gold_out.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"../datasets/final_queries_v1.7.json\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = dataset[2549:2553]\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out_v2 = df_out.reset_index(drop=True)\n",
    "df_out_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"templatize_test.json\")\n",
    "path.write_text(df_out_v2.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/dataset_pipeline/fq17_3/fq17_3-split_test.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = df.iloc[0]\n",
    "for c in df.columns:\n",
    "    print(c)\n",
    "    print(entry[c])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"../outputs/dataset_pipeline/fq17_3/fq17_3-split_test.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2].target_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments, AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sft_peft import format_prompt\n",
    "import sft_peft\n",
    "from prompts_template import BASE_MISTRAL_TEMPLATE, BASE_BASIC_INSTRUCTION\n",
    "from libwikidatallm.TemplateLLMQuerySender import TemplateLLMQuerySender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=\"[/INST]\", tokenizer=tokenizer)\n",
    "datasets = load_dataset(\"pandas\", data_files=\n",
    "                          {\n",
    "                              \"valid\": \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_valid.pkl\",\n",
    "                              \"train\": \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_train.pkl\",\n",
    "                              \"test\": \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_test.pkl\",\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    valid: Dataset({\n",
       "        features: ['basic_input', 'templated_input', 'target_raw', 'target_template', 'index'],\n",
       "        num_rows: 129\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['basic_input', 'templated_input', 'target_raw', 'target_template', 'index'],\n",
       "        num_rows: 1923\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['basic_input', 'templated_input', 'target_raw', 'target_template', 'index'],\n",
       "        num_rows: 513\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from SFTTrainer(Trainer) _prepare_non_packed_dataloader method\n",
    "def _prepare_non_packed_dataloader(\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    dataset_text_field,\n",
    "    max_seq_length,\n",
    "    formatting_func=None,\n",
    "    add_special_tokens=True,\n",
    "    remove_unused_columns=True,\n",
    "):\n",
    "    use_formatting_func = formatting_func is not None and dataset_text_field is None\n",
    "    # self._dataset_sanity_checked = False\n",
    "\n",
    "    # Inspired from: https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt\n",
    "    def tokenize(element):\n",
    "        outputs = tokenizer(\n",
    "            element[dataset_text_field] if not use_formatting_func else formatting_func(element),\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=max_seq_length,\n",
    "            return_overflowing_tokens=False,\n",
    "            return_length=False,\n",
    "        )\n",
    "\n",
    "        # if use_formatting_func and not self._dataset_sanity_checked:\n",
    "        #     if not isinstance(formatting_func(element), list):\n",
    "        #         raise ValueError(\n",
    "        #             \"The `formatting_func` should return a list of processed strings since it can lead to silent bugs.\"\n",
    "        #         )\n",
    "        #     else:\n",
    "        #         self._dataset_sanity_checked = True\n",
    "\n",
    "        return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n",
    "\n",
    "    signature_columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "\n",
    "    extra_colmuns = list(set(dataset.column_names) - set(signature_columns))\n",
    "\n",
    "    # if not remove_unused_columns and len(extra_colmuns) > 0:\n",
    "    #     warnings.warn(\n",
    "    #         \"You passed `remove_unused_columns=False` on a non-packed dataset. This might create some issues with the default collator and yield to errors. If you want to \"\n",
    "    #         f\"inspect dataset other columns (in this case {extra_colmuns}), you can subclass `DataCollatorForLanguageModeling` in case you used the default collator and create your own data collator in order to inspect the unused dataset columns.\"\n",
    "    #     )\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names if remove_unused_columns else None,\n",
    "        num_proc=1,\n",
    "        batch_size=32,\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = BASE_MISTRAL_TEMPLATE\n",
    "sft_peft.templater = TemplateLLMQuerySender(None, template, start_seq='[', end_seq=']')\n",
    "sft_peft.input_column = \"basic_input\"\n",
    "sft_peft.target_column = \"target_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = _prepare_non_packed_dataloader(tokenizer, datasets['test'], None, None, format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 513\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator(tokenized_dataset, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from sft_peft import main, parse_args\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args([\n",
    "    \"--model\", \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"--train-data\", \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_train.pkl\",\n",
    "    \"--target-column\", \"target_raw\",\n",
    "    \"--input-column\", \"basic_input\",\n",
    "    \"--valid-data\", \"../outputs/dataset_pipeline/fq17_3/fq17_3-split_valid.pkl\",\n",
    "    \"--start-tag\", \"[query]\",\n",
    "    \"--end-tag\", \"[/query]\",\n",
    "    \"--rvalue\", \"32\",\n",
    "    \"--lora-dropout\", \"0\",\n",
    "    \"--batch-size\", \"1\",\n",
    "    \"--gradient-accumulation\", str(4),\n",
    "    \"--packing\", \"0\",\n",
    "    \"--neft-tune-alpha\", \"0\",\n",
    "    \"--epochs\", \"1\",\n",
    "    \"--output\", \".\",\n",
    "    \"--save-name\", \"test_debug\",\n",
    "    \"--run-name\", f\"test_debug\",\n",
    "    \"--random-seed\", \"42\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "    print(list(zip(tokens, token_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁[', 733), ('/', 28748), ('INST', 16289), (']', 28793)]\n",
      "[('▁[', 733), ('/', 28748), ('INST', 16289), (']', 28793), ('▁[', 733), ('query', 3385), (']', 28793)]\n",
      "[('▁[', 733), ('/', 28748), ('INST', 16289), (']', 28793), ('▁[', 733), ('query', 3385), (']', 28793)]\n"
     ]
    }
   ],
   "source": [
    "print_tokens_with_ids(\"[/INST]\")\n",
    "print_tokens_with_ids(\"[/INST] [query]\")\n",
    "print_tokens_with_ids(\"[/INST] [query]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from prompts_template import ELABORATE_INSTRUCTION, BASE_MISTRAL_TEMPLATE\n",
    "from libwikidatallm.TemplateLLMQuerySender import TemplateLLMQuerySender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your assignment involves a two-step process:\n",
      "First, you will receive a task described in a single sentence. This task outlines what information you need to find or what question you need to answer.\n",
      "Then, you will create a SPARQL Query: Based on the task given, you will write a SPARQL query. SPARQL is a specialized query language used to retrieve and manipulate data stored in Resource Description Framework (RDF) format. Your goal is to craft a query that, when executed, fetches the data or answers required by the initial instruction.\n",
      "Make sure your SPARQL query is correctly formulated so that, upon execution, it produces the desired result matching the task's requirements.\n",
      "Answer this following instruction:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>description</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>start_with_SELECT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT ?property ?propertyType ?propertyLabel ...</td>\n",
       "      <td>Wikidata properties in numerical order</td>\n",
       "      <td>Counting stuff on Wikidata\\nAll Wikidata prope...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>267</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT ?id ?idLabel ?idDescription ?new{\\n?id ...</td>\n",
       "      <td>Wikidata properties excluding external IDs</td>\n",
       "      <td>Counting stuff on Wikidata\\nVariation of the a...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>296</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?...</td>\n",
       "      <td></td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of scientifi...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>197</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT (COUNT(DISTINCT ?article) AS ?count)\\nW...</td>\n",
       "      <td>Count of fictional characters</td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of fictional...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>203</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item...</td>\n",
       "      <td>Count of items with coordinate locations</td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of items wit...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>186</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  SELECT ?property ?propertyType ?propertyLabel ...   \n",
       "1  SELECT ?id ?idLabel ?idDescription ?new{\\n?id ...   \n",
       "2  SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?...   \n",
       "3  SELECT (COUNT(DISTINCT ?article) AS ?count)\\nW...   \n",
       "4  SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item...   \n",
       "\n",
       "                                  description  \\\n",
       "0      Wikidata properties in numerical order   \n",
       "1  Wikidata properties excluding external IDs   \n",
       "2                                               \n",
       "3               Count of fictional characters   \n",
       "4    Count of items with coordinate locations   \n",
       "\n",
       "                                             context  \\\n",
       "0  Counting stuff on Wikidata\\nAll Wikidata prope...   \n",
       "1  Counting stuff on Wikidata\\nVariation of the a...   \n",
       "2  Counting stuff on Wikidata\\nCount of scientifi...   \n",
       "3  Counting stuff on Wikidata\\nCount of fictional...   \n",
       "4  Counting stuff on Wikidata\\nCount of items wit...   \n",
       "\n",
       "                                              prompt  num_tokens  \\\n",
       "0  <s>[INST] <<SYS>>This is a conversation betwee...         267   \n",
       "1  <s>[INST] <<SYS>>This is a conversation betwee...         296   \n",
       "2  <s>[INST] <<SYS>>This is a conversation betwee...         197   \n",
       "3  <s>[INST] <<SYS>>This is a conversation betwee...         203   \n",
       "4  <s>[INST] <<SYS>>This is a conversation betwee...         186   \n",
       "\n",
       "   start_with_SELECT  \n",
       "0               True  \n",
       "1               True  \n",
       "2               True  \n",
       "3               True  \n",
       "4               True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"../datasets/final_queries_v1.7.json\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "print(ELABORATE_INSTRUCTION)\n",
    "\n",
    "templater = TemplateLLMQuerySender(llm=None, template_text=BASE_MISTRAL_TEMPLATE, start_seq=\"[\", end_seq=\"]\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_query = dataset['query'].loc[dataset['query'].map(len).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2739"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_prompts(query, templater:TemplateLLMQuerySender, template:str, start_tag:str = '`sparql\\n', end_tag:str = \"`\"):\n",
    "    start_tag = '`sparql\\n'\n",
    "    end_tag = \"`\"\n",
    "\n",
    "    x = templater.apply_template({\n",
    "        \"system_prompt\": template,\n",
    "        \"prompt\": \"\\\"\"+ (\"abigword \" * 20) +\"\\\"\"\n",
    "    })\n",
    "    x += f\"{start_tag}{query}{end_tag}\"\n",
    "    return x\n",
    "\n",
    "longest_prompt = prepare_prompts(longest_query, templater=templater, template=ELABORATE_INSTRUCTION, start_tag=\"`sparql\\n\", end_tag=\"`\")\n",
    "len(tokenizer(longest_prompt)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "import json\n",
    "import random\n",
    "from prompts_template import ELABORATE_INSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# folder_path = Path(\"../tests/scriptstests/data/compute_metrics_real_data_from_training_1\")\n",
    "folder_path = Path(\"../outputs/batch_run/learning_experiment_llama_3/models\")\n",
    "run_name = \"CL-7b-Instruct_oawbn8-rv256-rm1-ld0-bs2-ga8-gc1-p0-nta0-e3-ctx3072-qno-template-template-stsparql\"\n",
    "num_epochs = 3\n",
    "epochs_data = [json.loads((folder_path / f\"{run_name}_compute_metrics_{i}.json\").read_text()) for i in range(num_epochs)]\n",
    "\n",
    "len(epochs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['preds', 'labels', 'decoded_labels', 'decoded_preds'])\n"
     ]
    }
   ],
   "source": [
    "columns = epochs_data[0].keys()\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_answer_and_preds(row_number:int = 0):\n",
    "    print(f\"LABEL n°{row_number}\",\"=\" * 98)\n",
    "    print(epochs_data[0][\"decoded_labels\"][row_number])\n",
    "    print(\"EPOCH 0\",\"=\" * 100)\n",
    "    print(epochs_data[0][\"decoded_preds\"][row_number])\n",
    "    print(\"EPOCH 1\",\"-\" * 100)\n",
    "    print(epochs_data[1][\"decoded_preds\"][row_number])\n",
    "    print(\"EPOCH 2\",\"-\" * 100)\n",
    "    print(epochs_data[2][\"decoded_preds\"][row_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your assignment involves a two-step process:\n",
      "First, you will receive a task described in a single sentence. This task outlines what information you need to find or what question you need to answer.\n",
      "Then, you will create a SPARQL Query: Based on the task given, you will write a SPARQL query. SPARQL is a specialized query language used to retrieve and manipulate data stored in Resource Description Framework (RDF) format. Your goal is to craft a query that, when executed, fetches the data or answers required by the initial instruction.\n",
      "Make sure your SPARQL query is correctly formulated so that, upon execution, it produces the desired result matching the task's requirements.\n",
      "Answer this following instruction:\n"
     ]
    }
   ],
   "source": [
    "print(ELABORATE_INSTRUCTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL n°78 ==================================================================================================\n",
      "sparql]PREFIX bd: <http://www.bigdata.com/rdf#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ?item ?itemLabel ?itemDescription ?collectionLabel WHERE {\n",
      "?item wdt:[property:instance of] wd:[entity:oil painting]\n",
      "MINUS {?item wdt:[property:made from material] wd:[entity:oil paint]}\n",
      "OPTIONAL {?item wdt:[property:collection] ?collection}\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "}[/sparql]\n",
      "EPOCH 0 ====================================================================================================\n",
      "\n",
      ":: How name is creating lot-part process.\n",
      "\n",
      ", you will need a list to in a text sentence.\n",
      " sentence willlines the you you need to collect. create you you need to answer.\n",
      "Second, you will receive a newARQL query to\n",
      " on the information,, you will create a SPARQL query that\n",
      "ARQL is a queryized query language for to retrieve data manipulate data from in R Description Framework (RDF) tri.\n",
      " query is to find a SP that will when executed, willes the information you answers the by the task task.\n",
      "\n",
      " sure to queryARQL query is valid formattedulated and that it when execution, it returns the desired result. the task.s description.\n",
      "\n",
      " the question question:\n",
      "WhatFind all list of all fieldsings by are in in of oil paint.\" the medium.\" are to the museum that\n",
      "TasksINST> sparql]PREFIX bd: <http://www.bigdata.com/rdf#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ?item ?itemLabel ?collectionDescription ?collectionLabel WHERE {\n",
      "?item wdt:[property:instance of] wd:[entity:pil painting] .MINUS { ?item wdt:[property:material of material] wd:[entity:oil paint]}\n",
      "?IONAL {?item wdt:[property:collection] ?collection}\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "}[/sparql]\n",
      "ks list iteminginging isd itema item itemkakaaakakakakaqqqqkaqqqqlkqqkql]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]inkinkabelabelabel:::\n",
      "EPOCH 1 ----------------------------------------------------------------------------------------------------\n",
      "\n",
      ":: How name is creating lot-part process.\n",
      "\n",
      ", you will need a list to in a text sentence.\n",
      " sentence willlines the you you need to gather. create you you need to answer.\n",
      "Second, you will receive a newARQL query to a on the information,, you will create a SPARQL query that\n",
      "ARQL is a queryized query language for to retrieve data manipulate data from in R Description Framework (RDF) tri,\n",
      " query is to find a SP that will when executed, willes the information you answers the by the task task.\n",
      "\n",
      " sure to queryARQL query is valid formattedulated and that it when execution, it returns the desired result. the task.s description.\n",
      "\n",
      " the question question:\n",
      "WhatFind all list of all fieldsings by have in in of oil paint.\" the medium.\" are to the museum.\"\n",
      "]sINST: sparql]PREFIX bd: <http://www.bigdata.com/rdf#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ?item ?itemLabel ?collectionDescription ?collectionLabel WHERE {\n",
      "?item wdt:[property:instance of] wd:[entity:pil painting] .MINUS { ?item wdt:[property:material from material] wd:[entity:oil paint]}\n",
      "OPTIONAL {?item wdt:[property:collection] ?collection}\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "}[/sparql]\n",
      " item itemvveseses ap ispeskpaaesksksqqqlql]kakaqqlql]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]abelabelabel]abelabelabelabel::\n",
      "EPOCH 2 ----------------------------------------------------------------------------------------------------\n",
      "\n",
      ":: How name is creating lot-part process.\n",
      "\n",
      ", you will need a list to in a text sentence.\n",
      " sentence willlines the you you need to gather. create you you need to answer.\n",
      "Second, you will receive a newARQL query to a on the information,, you will create a SPARQL query that\n",
      "ARQL is a queryized query language for to retrieve data manipulate data from in R Description Framework (RDF) tri,\n",
      " query is to find a SP that will when executed, willes the information you answers the by the task task.\n",
      "\n",
      " sure to queryARQL query is valid formattedulated and that it when execution, it returns the desired result. the task.s description.\n",
      "\n",
      " the question question:\n",
      "WhatFind all list of all fieldsings by have in in of oil paint.\" the medium.\" are to the museum.\"\n",
      "]sINST> sparql]PREFIX bd: <http://www.bigdata.com/rdf#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ?item ?itemLabel ?collectionDescription ?collectionLabel WHERE {\n",
      "?item wdt:[property:instance of] wd:[entity:pil painting] .MINUS { ?item wdt:[property:made from material] wd:[entity:oil paint]}\n",
      "OPTIONAL {?item wdt:[property:collection] ?collection}\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "}[/sparql]\n",
      "vvveses]] isparesppaparesenenqqqlql]]]qqlqlql]]]ql]]]]]]]]]]]q]]]]]]]]]]]]]]]qqq]qqqqqqq\n"
     ]
    }
   ],
   "source": [
    "n_rows = len(epochs_data[0][\"decoded_preds\"])\n",
    "\n",
    "print_label_answer_and_preds(random.randint(0, n_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, load_peft_weights\n",
    "from prompts_template import ELABORATE_INSTRUCTION, BASE_MISTRAL_TEMPLATE\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "adapter_path = \"../outputs/batch_run/learning_experiment_long_epochs_4/models/M-7B-I-v0.2_oawtor-rv256-rm1-ld0-bs2-ga8-gc1-p0-nta0-e10-ctx3072-qno-template-template-stsparql_adapters\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecbc7bb265d4233b28cd54e3c0d3207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(model, adapter_path, is_trainable=False)\n",
    "except RuntimeError as e: # RuntimeError => Might be padding token is left in the embedding when saved\n",
    "    d_adpt = load_peft_weights(adapter_path)\n",
    "    len_d_adpt = len(d_adpt.get('base_model.model.lm_head.weight', 0))\n",
    "    if len_d_adpt != 0 and len_d_adpt != len(tokenizer):\n",
    "        model.resize_token_embeddings(len(tokenizer) + 1)\n",
    "        model = PeftModel.from_pretrained(model, adapter_path, is_trainable=False)\n",
    "    else:\n",
    "        raise e\n",
    "except Exception as e:\n",
    "    try:\n",
    "        model = PeftModel.from_pretrained(model, os.path.abspath(adapter_path), is_trainable=False)\n",
    "    except RuntimeError as e: # RuntimeError => Might be padding token is left in the embedding when saved\n",
    "        d_adpt = load_peft_weights(adapter_path)\n",
    "        len_d_adpt = len(d_adpt.get('base_model.model.lm_head.weight', 0))\n",
    "        if len_d_adpt != 0 and len_d_adpt != len(tokenizer):\n",
    "            model.resize_token_embeddings(len(tokenizer) + 1)\n",
    "            model = PeftModel.from_pretrained(model, os.path.abspath(adapter_path), is_trainable=False)\n",
    "        else:\n",
    "            raise e\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32001"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_adpt = load_peft_weights(adapter_path)\n",
    "len(d_adpt.get('base_model.model.lm_head.weight', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model.model.lm_head.weight',\n",
       " 'base_model.model.model.embed_tokens.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight',\n",
       " 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight',\n",
       " 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(d_adpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32000, 4096)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "modified_tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified_tokenizer.pad_token='[PAD]'\n",
      "modified_tokenizer.pad_token_id=32000\n"
     ]
    }
   ],
   "source": [
    "print(f\"{modified_tokenizer.pad_token=}\")\n",
    "print(f\"{modified_tokenizer.pad_token_id=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from data_utils import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from generate_finetune_dataset import keep_working_queries\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>description</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>start_with_SELECT</th>\n",
       "      <th>query_templated</th>\n",
       "      <th>basic_prompt</th>\n",
       "      <th>basic_num_tokens</th>\n",
       "      <th>basic_result</th>\n",
       "      <th>...</th>\n",
       "      <th>basic_is_skipped</th>\n",
       "      <th>basic_is_prompt_too_long</th>\n",
       "      <th>templated_prompt</th>\n",
       "      <th>templated_num_tokens</th>\n",
       "      <th>templated_result</th>\n",
       "      <th>templated_full_answer</th>\n",
       "      <th>templated_is_skipped</th>\n",
       "      <th>templated_is_prompt_too_long</th>\n",
       "      <th>execution</th>\n",
       "      <th>executed_query</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT ?property ?propertyType ?propertyLabel ...</td>\n",
       "      <td>Wikidata properties in numerical order</td>\n",
       "      <td>Counting stuff on Wikidata\\nAll Wikidata prope...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>267</td>\n",
       "      <td>True</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"SELECT ?property ?propertyTyp...</td>\n",
       "      <td>226</td>\n",
       "      <td>[\"Write a SparQL query to retrieve Wikidata pr...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"PREFIX bd: &lt;http://www.bigdat...</td>\n",
       "      <td>289</td>\n",
       "      <td>[\"Write a SparQL query to retrieve all Wikidat...</td>\n",
       "      <td>{'content': ' 1. \"Write a SparQL query to retr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'property': {'type': 'uri', 'value': 'http:/...</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT ?id ?idLabel ?idDescription ?new{\\n?id ...</td>\n",
       "      <td>Wikidata properties excluding external IDs</td>\n",
       "      <td>Counting stuff on Wikidata\\nVariation of the a...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>296</td>\n",
       "      <td>True</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"SELECT ?id ?idLabel ?idDescri...</td>\n",
       "      <td>259</td>\n",
       "      <td>[\"Write a SparQL query to retrieve the ID, lab...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"PREFIX bd: &lt;http://www.bigdat...</td>\n",
       "      <td>322</td>\n",
       "      <td>[\"Write a SparQL query to retrieve the ID, lab...</td>\n",
       "      <td>{'content': ' 1. \"Write a SparQL query to retr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'id': {'type': 'uri', 'value': 'http://www.w...</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?...</td>\n",
       "      <td></td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of scientifi...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>197</td>\n",
       "      <td>True</td>\n",
       "      <td>PREFIX wd: &lt;http://www.wikidata.org/entity/&gt;\\n...</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"SELECT (COUNT(?article) AS ?c...</td>\n",
       "      <td>159</td>\n",
       "      <td>[\"Find the total number of scientific articles...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"PREFIX wd: &lt;http://www.wikida...</td>\n",
       "      <td>201</td>\n",
       "      <td>[\"Write a SparQL query to retrieve the count o...</td>\n",
       "      <td>{'content': ' 1. \"Write a SparQL query to retr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>timeout</td>\n",
       "      <td>PREFIX wd: &lt;http://www.wikidata.org/entity/&gt;\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT (COUNT(DISTINCT ?article) AS ?count)\\nW...</td>\n",
       "      <td>Count of fictional characters</td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of fictional...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>203</td>\n",
       "      <td>True</td>\n",
       "      <td>PREFIX wd: &lt;http://www.wikidata.org/entity/&gt;\\n...</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"SELECT (COUNT(DISTINCT ?artic...</td>\n",
       "      <td>165</td>\n",
       "      <td>[\"Find the number of distinct fictional charac...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"PREFIX wd: &lt;http://www.wikida...</td>\n",
       "      <td>210</td>\n",
       "      <td>[\"Write a query to find the total number of di...</td>\n",
       "      <td>{'content': ' 1. \"Write a query to find the to...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'count': {'datatype': 'http://www.w3.org/200...</td>\n",
       "      <td>PREFIX wd: &lt;http://www.wikidata.org/entity/&gt;\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item...</td>\n",
       "      <td>Count of items with coordinate locations</td>\n",
       "      <td>Counting stuff on Wikidata\\nCount of items wit...</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;This is a conversation betwee...</td>\n",
       "      <td>186</td>\n",
       "      <td>True</td>\n",
       "      <td>PREFIX wdt: &lt;http://www.wikidata.org/prop/dire...</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"SELECT (COUNT(?item) AS ?coun...</td>\n",
       "      <td>148</td>\n",
       "      <td>[\"How many items in Wikidata have recorded coo...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;s&gt;[INST] QUERY=\"PREFIX wdt: &lt;http://www.wikid...</td>\n",
       "      <td>172</td>\n",
       "      <td>[\"Write a SparQL query to retrieve the number ...</td>\n",
       "      <td>{'content': ' 1. \"Write a SparQL query to retr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'count': {'datatype': 'http://www.w3.org/200...</td>\n",
       "      <td>PREFIX wdt: &lt;http://www.wikidata.org/prop/dire...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   query  \\\n",
       "index                                                      \n",
       "0      SELECT ?property ?propertyType ?propertyLabel ...   \n",
       "1      SELECT ?id ?idLabel ?idDescription ?new{\\n?id ...   \n",
       "2      SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?...   \n",
       "3      SELECT (COUNT(DISTINCT ?article) AS ?count)\\nW...   \n",
       "4      SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item...   \n",
       "\n",
       "                                      description  \\\n",
       "index                                               \n",
       "0          Wikidata properties in numerical order   \n",
       "1      Wikidata properties excluding external IDs   \n",
       "2                                                   \n",
       "3                   Count of fictional characters   \n",
       "4        Count of items with coordinate locations   \n",
       "\n",
       "                                                 context  \\\n",
       "index                                                      \n",
       "0      Counting stuff on Wikidata\\nAll Wikidata prope...   \n",
       "1      Counting stuff on Wikidata\\nVariation of the a...   \n",
       "2      Counting stuff on Wikidata\\nCount of scientifi...   \n",
       "3      Counting stuff on Wikidata\\nCount of fictional...   \n",
       "4      Counting stuff on Wikidata\\nCount of items wit...   \n",
       "\n",
       "                                                  prompt  num_tokens  \\\n",
       "index                                                                  \n",
       "0      <s>[INST] <<SYS>>This is a conversation betwee...         267   \n",
       "1      <s>[INST] <<SYS>>This is a conversation betwee...         296   \n",
       "2      <s>[INST] <<SYS>>This is a conversation betwee...         197   \n",
       "3      <s>[INST] <<SYS>>This is a conversation betwee...         203   \n",
       "4      <s>[INST] <<SYS>>This is a conversation betwee...         186   \n",
       "\n",
       "       start_with_SELECT                                    query_templated  \\\n",
       "index                                                                         \n",
       "0                   True  PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...   \n",
       "1                   True  PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...   \n",
       "2                   True  PREFIX wd: <http://www.wikidata.org/entity/>\\n...   \n",
       "3                   True  PREFIX wd: <http://www.wikidata.org/entity/>\\n...   \n",
       "4                   True  PREFIX wdt: <http://www.wikidata.org/prop/dire...   \n",
       "\n",
       "                                            basic_prompt  basic_num_tokens  \\\n",
       "index                                                                        \n",
       "0      <s>[INST] QUERY=\"SELECT ?property ?propertyTyp...               226   \n",
       "1      <s>[INST] QUERY=\"SELECT ?id ?idLabel ?idDescri...               259   \n",
       "2      <s>[INST] QUERY=\"SELECT (COUNT(?article) AS ?c...               159   \n",
       "3      <s>[INST] QUERY=\"SELECT (COUNT(DISTINCT ?artic...               165   \n",
       "4      <s>[INST] QUERY=\"SELECT (COUNT(?item) AS ?coun...               148   \n",
       "\n",
       "                                            basic_result  ...  \\\n",
       "index                                                     ...   \n",
       "0      [\"Write a SparQL query to retrieve Wikidata pr...  ...   \n",
       "1      [\"Write a SparQL query to retrieve the ID, lab...  ...   \n",
       "2      [\"Find the total number of scientific articles...  ...   \n",
       "3      [\"Find the number of distinct fictional charac...  ...   \n",
       "4      [\"How many items in Wikidata have recorded coo...  ...   \n",
       "\n",
       "      basic_is_skipped  basic_is_prompt_too_long  \\\n",
       "index                                              \n",
       "0                False                     False   \n",
       "1                False                     False   \n",
       "2                False                     False   \n",
       "3                False                     False   \n",
       "4                False                     False   \n",
       "\n",
       "                                        templated_prompt templated_num_tokens  \\\n",
       "index                                                                           \n",
       "0      <s>[INST] QUERY=\"PREFIX bd: <http://www.bigdat...                  289   \n",
       "1      <s>[INST] QUERY=\"PREFIX bd: <http://www.bigdat...                  322   \n",
       "2      <s>[INST] QUERY=\"PREFIX wd: <http://www.wikida...                  201   \n",
       "3      <s>[INST] QUERY=\"PREFIX wd: <http://www.wikida...                  210   \n",
       "4      <s>[INST] QUERY=\"PREFIX wdt: <http://www.wikid...                  172   \n",
       "\n",
       "                                        templated_result  \\\n",
       "index                                                      \n",
       "0      [\"Write a SparQL query to retrieve all Wikidat...   \n",
       "1      [\"Write a SparQL query to retrieve the ID, lab...   \n",
       "2      [\"Write a SparQL query to retrieve the count o...   \n",
       "3      [\"Write a query to find the total number of di...   \n",
       "4      [\"Write a SparQL query to retrieve the number ...   \n",
       "\n",
       "                                   templated_full_answer templated_is_skipped  \\\n",
       "index                                                                           \n",
       "0      {'content': ' 1. \"Write a SparQL query to retr...                False   \n",
       "1      {'content': ' 1. \"Write a SparQL query to retr...                False   \n",
       "2      {'content': ' 1. \"Write a SparQL query to retr...                False   \n",
       "3      {'content': ' 1. \"Write a query to find the to...                False   \n",
       "4      {'content': ' 1. \"Write a SparQL query to retr...                False   \n",
       "\n",
       "       templated_is_prompt_too_long  \\\n",
       "index                                 \n",
       "0                             False   \n",
       "1                             False   \n",
       "2                             False   \n",
       "3                             False   \n",
       "4                             False   \n",
       "\n",
       "                                               execution  \\\n",
       "index                                                      \n",
       "0      [{'property': {'type': 'uri', 'value': 'http:/...   \n",
       "1      [{'id': {'type': 'uri', 'value': 'http://www.w...   \n",
       "2                                                timeout   \n",
       "3      [{'count': {'datatype': 'http://www.w3.org/200...   \n",
       "4      [{'count': {'datatype': 'http://www.w3.org/200...   \n",
       "\n",
       "                                          executed_query  \n",
       "index                                                     \n",
       "0      PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...  \n",
       "1      PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...  \n",
       "2      PREFIX wd: <http://www.wikidata.org/entity/>\\n...  \n",
       "3      PREFIX wd: <http://www.wikidata.org/entity/>\\n...  \n",
       "4      PREFIX wdt: <http://www.wikidata.org/prop/dire...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(\"../outputs/dataset_pipeline/fq17_3/fq17_3-generated_prompt-executed.parquet.gzip\")\n",
    "if not path.exists():\n",
    "    raise FileExistsError(f\"No file was found at this path: {path}\")\n",
    "\n",
    "dataset = load_dataset(path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'description', 'context', 'prompt', 'num_tokens',\n",
       "       'start_with_SELECT', 'query_templated', 'basic_prompt',\n",
       "       'basic_num_tokens', 'basic_result', 'basic_full_answer',\n",
       "       'basic_is_skipped', 'basic_is_prompt_too_long', 'templated_prompt',\n",
       "       'templated_num_tokens', 'templated_result', 'templated_full_answer',\n",
       "       'templated_is_skipped', 'templated_is_prompt_too_long', 'execution',\n",
       "       'executed_query'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>templated_result</th>\n",
       "      <th>query_templated</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"Write a SparQL query to retrieve all Wikidat...</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"Write a SparQL query to retrieve the ID, lab...</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"Write a query to find the total number of di...</td>\n",
       "      <td>PREFIX wd: &lt;http://www.wikidata.org/entity/&gt;\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"Write a SparQL query to retrieve the number ...</td>\n",
       "      <td>PREFIX wdt: &lt;http://www.wikidata.org/prop/dire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[\"Write a SparQL query to retrieve the number ...</td>\n",
       "      <td>PREFIX bd: &lt;http://www.bigdata.com/rdf#&gt;\\nPREF...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        templated_result  \\\n",
       "index                                                      \n",
       "0      [\"Write a SparQL query to retrieve all Wikidat...   \n",
       "1      [\"Write a SparQL query to retrieve the ID, lab...   \n",
       "3      [\"Write a query to find the total number of di...   \n",
       "4      [\"Write a SparQL query to retrieve the number ...   \n",
       "5      [\"Write a SparQL query to retrieve the number ...   \n",
       "\n",
       "                                         query_templated  \n",
       "index                                                     \n",
       "0      PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...  \n",
       "1      PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...  \n",
       "3      PREFIX wd: <http://www.wikidata.org/entity/>\\n...  \n",
       "4      PREFIX wdt: <http://www.wikidata.org/prop/dire...  \n",
       "5      PREFIX bd: <http://www.bigdata.com/rdf#>\\nPREF...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped_dataset = keep_working_queries(dataset)\n",
    "\n",
    "columns_to_take = [\n",
    "    \"templated_result\",\n",
    "    \"query_templated\",\n",
    "]\n",
    "\n",
    "dataset_for_tokenization = dropped_dataset[columns_to_take]\n",
    "dataset_for_tokenization.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_concat_dataset)=2294959\n"
     ]
    }
   ],
   "source": [
    "all_concat_dataset = dataset_for_tokenization.apply(lambda x: x['templated_result'][0].strip() + x[\"query_templated\"].strip(), axis=1).sum()\n",
    "print(f\"{len(all_concat_dataset)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(all_concat_dataset)\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens.tokens())=824341\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(tokens.tokens())=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry n°353\n",
      "----- Prompts From Original -----\n",
      "\"Write a SPARQL query to retrieve all books with a title, illustrator, publisher, and publication date, but allow for the possibility that some books may not have an illustrator or publisher listed.\"\n",
      "\"Create a SPARQL query to find all books with a title, optional illustrator and publisher information, and a publication date.\"\n",
      "\"Generate a SPARQL query to retrieve books with required title, publication date, and optional illustrator and publisher information.\"\n",
      "----- Original Query -----\n",
      "SELECT ?book ?title ?illustratorLabel ?publisherLabel ?published\n",
      "WHERE\n",
      "{\n",
      "?book wdt:P50 wd:Q35610. # required wdt:P50\n",
      "OPTIONAL { # match all or none from group:\n",
      "?book wdt:P1476 ?title; # required wdt:P1476\n",
      "wdt:P110 ?illustrator; # required wdt:P110\n",
      "wdt:P123 ?publisher; # required wdt:P123\n",
      "wdt:P577 ?published. # required wdt:P577\n",
      "}\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "} \n",
      "\n",
      "----- Prompts From Templated -----\n",
      "\"Write a SPARQL query to retrieve books authored by Arthur Conan Doyle, along with their titles, illustrators, publishers, and publication dates if available.\"\n",
      "\"Create a SPARQL query that returns all books with an author of 'Arthur Conan Doyle', as well as their optional title, illustrator, publisher, and publication date information.\"\n",
      "----- Templated Query -----\n",
      "PREFIX bd: <http://www.bigdata.com/rdf#>\n",
      "PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ?book ?title ?illustratorLabel ?publisherLabel ?published\n",
      "WHERE\n",
      "{\n",
      "?book wdt:[property:author] wd:[entity:Arthur Conan Doyle]. # required wdt:[property:author]\n",
      "OPTIONAL { # match all or none from group:\n",
      "?book wdt:[property:title] ?title; # required wdt:[property:title]\n",
      "wdt:[property:illustrator] ?illustrator; # required wdt:[property:illustrator]\n",
      "wdt:[property:publisher] ?publisher; # required wdt:[property:publisher]\n",
      "wdt:[property:publication date] ?published. # required wdt:[property:publication date]\n",
      "}\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "entry_iloc_id = random.randint(0, len(dropped_dataset) - 1)\n",
    "entry = dropped_dataset.iloc[entry_iloc_id]\n",
    "print(f\"Entry n°{entry_iloc_id}\")\n",
    "print(\"----- Prompts From Original -----\")\n",
    "print(\"\\n\".join(entry[\"basic_result\"]))\n",
    "print(\"----- Original Query -----\")\n",
    "print(entry[\"query\"], \"\\n\")\n",
    "print(\"----- Prompts From Templated -----\")\n",
    "print(\"\\n\".join(entry[\"templated_result\"]))\n",
    "print(\"----- Templated Query -----\")\n",
    "print(entry[\"query_templated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'description', 'context', 'prompt', 'num_tokens',\n",
       "       'start_with_SELECT', 'query_templated', 'basic_prompt',\n",
       "       'basic_num_tokens', 'basic_result', 'basic_full_answer',\n",
       "       'basic_is_skipped', 'basic_is_prompt_too_long', 'templated_prompt',\n",
       "       'templated_num_tokens', 'templated_result', 'templated_full_answer',\n",
       "       'templated_is_skipped', 'templated_is_prompt_too_long', 'execution',\n",
       "       'executed_query'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] QUERY=\"PREFIX bd: <http://www.bigdata.com/rdf#>\n",
      "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
      "\n",
      "SELECT ?property ?propertyType ?propertyLabel ?propertyDescription WHERE {\n",
      "?property wikibase:propertyType ?propertyType .\n",
      "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
      "} ORDER BY ASC(xsd:integer(STRAFTER(STR(?property), 'P')))\" DESCRIPTION=\"Wikidata properties in numerical order\" CONTEXT=\"Counting stuff on Wikidata\n",
      "All Wikidata properties with label and description, ordered numerically\n",
      "Adapted from one of the Query Service Examples\" Read QUERY, DESCRIPTION, and CONTEXT. There is a machine capable of writing the given SparQL QUERY if we provide the correct prompt. A prompt should be at least one sentence and no longer than five sentences. Each prompt must be enclosed in quotation marks. Provide a list of three prompts that would elicit the QUERY. [/INST]\n",
      "['\"Write a SparQL query to retrieve all Wikidata properties with their labels and descriptions, ordered numerically.\"', '\"Create a query that returns the property name, type, label, and description for each Wikidata property in numerical order.\"', '\"Generate a SparQL query to extract the label and description of Wikidata properties, sorted by their numerical identifiers. Use prefixes for Big Data, XML Schema, and Wikibase ontology.\"']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['templated_prompt'].iloc[0])\n",
    "print(dataset['templated_result'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query\n",
      "description\n",
      "context\n",
      "prompt\n",
      "num_tokens\n",
      "start_with_SELECT\n",
      "query_templated\n",
      "basic_prompt\n",
      "basic_num_tokens\n",
      "basic_result\n",
      "basic_full_answer\n",
      "basic_is_skipped\n",
      "basic_is_prompt_too_long\n",
      "templated_prompt\n",
      "templated_num_tokens\n",
      "templated_result\n",
      "templated_full_answer\n",
      "templated_is_skipped\n",
      "templated_is_prompt_too_long\n",
      "execution\n",
      "executed_query\n"
     ]
    }
   ],
   "source": [
    "QUERY=\"PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "\n",
    "SELECT ?property ?propertyType ?propertyLabel ?propertyDescription WHERE {\n",
    "?property wikibase:propertyType ?propertyType .\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "} ORDER BY ASC(xsd:integer(STRAFTER(STR(?property), 'P')))\" DESCRIPTION=\"Wikidata properties in numerical order\" CONTEXT=\"Counting stuff on Wikidata\n",
    "All Wikidata properties with label and description, ordered numerically\n",
    "Adapted from one of the Query Service Examples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(Path(\"../modules\").absolute().__str__())\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<s> [INST] Your assignment involves a two-step process:                                                                                    \n",
    "First, you will receive a task described in a single sentence. This task outlines what information you need to find or what question you need to answer.                        \n",
    "Then, you will create a SPARQL Query: Based on the task given, you will write a SPARQL query. SPARQL is a specialized query language used to retrieve and manipulate data stored\n",
    " in Resource Description Framework (RDF) format. Your goal is to craft a query that, when executed, fetches the data or answers required by the initial instruction.\n",
    "Make sure your SPARQL query is correctly formulated so that, upon execution, it produces the desired result matching the task's requirements.\n",
    "Answer this following instruction:                                                      \n",
    "\"Write a SparQL query to find the top 10 properties that connect warriors or military personnel to military organizations in Wikidata, ordered by frequency of usage.\"\n",
    "[/INST] [sparql]PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "PREFIX hint: <http://www.bigdata.com/queryHints#>\n",
    "PREFIX wd: <http://www.wikidata.org/entity/>                                                                                                                                    \n",
    "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "PREFIX wikibase: <http://wikiba.se/ontology#>                                                                                                                                   \n",
    "                                            \n",
    "SELECT ?count ?property ?propertyLabel ?samp WHERE {                                                                                                                            \n",
    "{                                                                                       \n",
    "SELECT (COUNT(?item) AS ?count) ?property (SAMPLE(?item) AS ?samp) WHERE {                                                                                                      \n",
    "hint:Query hint:optimizer \"None\".  \n",
    "?org_class wdt:[property:subclass of]* wd:[entity:armed organization].                                                                                                          \n",
    "?org wdt:[property:instance of]? ?org_class .\n",
    "?item ?prop ?org .                                                                      \n",
    "?item wdt:[property:occupation] ?role .\n",
    "?role wdt:[property:subclass of]* wd:[entity:warrior] .                                                                                                                         \n",
    "?property wikibase:directClaim ?prop .\n",
    "} GROUP BY ?property                                                                                                                                                            \n",
    "}                                                                                                                                                                               \n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}                                 \n",
    "ORDER BY DESC (?count)[/sparql]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/CodeLlama-7b-Instruct-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 518, 29914, 25580, 29962], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"[/INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 1, 29871, 518, 25580, 29962, 3575, 12827, 20789, 263, 1023, 29899, 10568, 1889, 29901, 462, 462, 462, 462, 462, 268, 13, 6730, 29892, 366, 674, 7150, 263, 3414, 5439, 297, 263, 2323, 10541, 29889, 910, 3414, 714, 9012, 825, 2472, 366, 817, 304, 1284, 470, 825, 1139, 366, 817, 304, 1234, 29889, 462, 308, 13, 11760, 29892, 366, 674, 1653, 263, 10937, 1718, 2239, 13641, 29901, 16564, 373, 278, 3414, 2183, 29892, 366, 674, 2436, 263, 10937, 1718, 2239, 2346, 29889, 10937, 1718, 2239, 338, 263, 4266, 1891, 2346, 4086, 1304, 304, 10563, 322, 26749, 848, 6087, 13, 297, 18981, 12953, 16657, 313, 29934, 4037, 29897, 3402, 29889, 3575, 7306, 338, 304, 25554, 263, 2346, 393, 29892, 746, 8283, 29892, 6699, 267, 278, 848, 470, 6089, 3734, 491, 278, 2847, 15278, 29889, 13, 9984, 1854, 596, 10937, 1718, 2239, 2346, 338, 5149, 883, 7964, 577, 393, 29892, 2501, 8225, 29892, 372, 13880, 278, 7429, 1121, 9686, 278, 3414, 29915, 29879, 11780, 29889, 13, 22550, 445, 1494, 15278, 29901, 462, 462, 462, 539, 13, 29908, 6113, 263, 317, 862, 2239, 2346, 304, 1284, 278, 2246, 29871, 29896, 29900, 4426, 393, 4511, 1370, 28739, 470, 9121, 24127, 304, 9121, 25700, 297, 3772, 333, 532, 29892, 10372, 491, 10868, 310, 8744, 1213, 13, 29961, 29914, 25580, 29962, 518, 29879, 862, 1519, 29962, 15094, 25634, 289, 29881, 29901, 529, 1124, 597, 1636, 29889, 3752, 1272, 29889, 510, 29914, 29878, 2176, 29937, 29958, 13, 15094, 25634, 13182, 29901, 529, 1124, 597, 1636, 29889, 3752, 1272, 29889, 510, 29914, 1972, 29950, 9466, 29937, 29958, 13, 15094, 25634, 281, 29881, 29901, 529, 1124, 597, 1636, 29889, 2851, 333, 532, 29889, 990, 29914, 10041, 3779, 462, 462, 462, 462, 462, 462, 462, 462, 268, 13, 15094, 25634, 281, 6008, 29901, 529, 1124, 597, 1636, 29889, 2851, 333, 532, 29889, 990, 29914, 7728, 29914, 11851, 3779, 13, 15094, 25634, 281, 638, 747, 559, 29901, 529, 1124, 597, 2851, 16912, 29889, 344, 29914, 609, 3002, 29937, 29958, 462, 462, 462, 462, 462, 462, 462, 462, 1678, 13, 462, 462, 632, 13, 6404, 1577, 2798, 1577, 6799, 1577, 6799, 4775, 1577, 29879, 1160, 5754, 426, 462, 462, 462, 462, 462, 462, 462, 632, 13, 29912, 462, 462, 462, 462, 462, 4706, 13, 6404, 313, 18736, 10780, 667, 29897, 3339, 1577, 2798, 29897, 1577, 6799, 313, 8132, 3580, 1307, 10780, 667, 29897, 3339, 1577, 29879, 1160, 29897, 5754, 426, 462, 462, 462, 462, 462, 462, 539, 13, 29882, 524, 29901, 3010, 13182, 29901, 20640, 3950, 376, 8516, 1642, 259, 13, 29973, 990, 29918, 1990, 281, 6008, 10834, 6799, 29901, 1491, 1990, 310, 14178, 281, 29881, 10834, 10041, 29901, 279, 2168, 13013, 1822, 462, 462, 462, 462, 462, 462, 965, 13, 29973, 990, 281, 6008, 10834, 6799, 29901, 8758, 310, 29962, 29973, 1577, 990, 29918, 1990, 869, 13, 29973, 667, 1577, 7728, 1577, 990, 869, 462, 462, 462, 462, 539, 13, 29973, 667, 281, 6008, 10834, 6799, 29901, 26601, 29962, 1577, 12154, 869, 13, 29973, 12154, 281, 6008, 10834, 6799, 29901, 1491, 1990, 310, 14178, 281, 29881, 10834, 10041, 29901, 4495, 13479, 29962, 869, 462, 462, 462, 462, 462, 462, 462, 3986, 13, 29973, 6799, 281, 638, 747, 559, 29901, 11851, 29907, 8342, 1577, 7728, 869, 13, 29913, 15345, 6770, 1577, 6799, 462, 462, 462, 462, 462, 462, 462, 462, 462, 632, 13, 29913, 462, 462, 462, 462, 462, 462, 462, 462, 462, 462, 18884, 13, 6304, 19059, 281, 638, 747, 559, 29901, 1643, 426, 289, 29881, 29901, 5509, 4736, 281, 638, 747, 559, 29901, 11675, 14704, 20656, 29949, 29918, 29931, 19453, 29965, 10461, 1402, 264, 1642, 500, 13, 29913, 462, 462, 29871, 13, 22364, 6770, 23050, 22308, 2798, 9601, 29914, 29879, 862, 1519, 29962], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 => <s>\n",
      "1 => <s>\n",
      "29871 => \n",
      "518 => [\n",
      "25580 => INST\n",
      "29962 => ]\n",
      "3575 => Your\n",
      "12827 => assignment\n",
      "20789 => involves\n",
      "263 => a\n",
      "1023 => two\n",
      "29899 => -\n",
      "10568 => step\n",
      "1889 => process\n",
      "29901 => :\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "268 =>    \n",
      "13 => \n",
      "\n",
      "6730 => First\n",
      "29892 => ,\n",
      "366 => you\n",
      "674 => will\n",
      "7150 => receive\n",
      "263 => a\n",
      "3414 => task\n",
      "5439 => described\n",
      "297 => in\n",
      "263 => a\n",
      "2323 => single\n",
      "10541 => sentence\n",
      "29889 => .\n",
      "910 => This\n",
      "3414 => task\n",
      "714 => out\n",
      "9012 => lines\n",
      "825 => what\n",
      "2472 => information\n",
      "366 => you\n",
      "817 => need\n",
      "304 => to\n",
      "1284 => find\n",
      "470 => or\n",
      "825 => what\n",
      "1139 => question\n",
      "366 => you\n",
      "817 => need\n",
      "304 => to\n",
      "1234 => answer\n",
      "29889 => .\n",
      "462 =>                \n",
      "308 =>        \n",
      "13 => \n",
      "\n",
      "11760 => Then\n",
      "29892 => ,\n",
      "366 => you\n",
      "674 => will\n",
      "1653 => create\n",
      "263 => a\n",
      "10937 => SP\n",
      "1718 => AR\n",
      "2239 => QL\n",
      "13641 => Query\n",
      "29901 => :\n",
      "16564 => Based\n",
      "373 => on\n",
      "278 => the\n",
      "3414 => task\n",
      "2183 => given\n",
      "29892 => ,\n",
      "366 => you\n",
      "674 => will\n",
      "2436 => write\n",
      "263 => a\n",
      "10937 => SP\n",
      "1718 => AR\n",
      "2239 => QL\n",
      "2346 => query\n",
      "29889 => .\n",
      "10937 => SP\n",
      "1718 => AR\n",
      "2239 => QL\n",
      "338 => is\n",
      "263 => a\n",
      "4266 => special\n",
      "1891 => ized\n",
      "2346 => query\n",
      "4086 => language\n",
      "1304 => used\n",
      "304 => to\n",
      "10563 => retrieve\n",
      "322 => and\n",
      "26749 => manipulate\n",
      "848 => data\n",
      "6087 => stored\n",
      "13 => \n",
      "\n",
      "297 => in\n",
      "18981 => Resource\n",
      "12953 => Description\n",
      "16657 => Framework\n",
      "313 => (\n",
      "29934 => R\n",
      "4037 => DF\n",
      "29897 => )\n",
      "3402 => format\n",
      "29889 => .\n",
      "3575 => Your\n",
      "7306 => goal\n",
      "338 => is\n",
      "304 => to\n",
      "25554 => craft\n",
      "263 => a\n",
      "2346 => query\n",
      "393 => that\n",
      "29892 => ,\n",
      "746 => when\n",
      "8283 => executed\n",
      "29892 => ,\n",
      "6699 => fetch\n",
      "267 => es\n",
      "278 => the\n",
      "848 => data\n",
      "470 => or\n",
      "6089 => answers\n",
      "3734 => required\n",
      "491 => by\n",
      "278 => the\n",
      "2847 => initial\n",
      "15278 => instruction\n",
      "29889 => .\n",
      "13 => \n",
      "\n",
      "9984 => Make\n",
      "1854 => sure\n",
      "596 => your\n",
      "10937 => SP\n",
      "1718 => AR\n",
      "2239 => QL\n",
      "2346 => query\n",
      "338 => is\n",
      "5149 => correctly\n",
      "883 => form\n",
      "7964 => ulated\n",
      "577 => so\n",
      "393 => that\n",
      "29892 => ,\n",
      "2501 => upon\n",
      "8225 => execution\n",
      "29892 => ,\n",
      "372 => it\n",
      "13880 => produces\n",
      "278 => the\n",
      "7429 => desired\n",
      "1121 => result\n",
      "9686 => matching\n",
      "278 => the\n",
      "3414 => task\n",
      "29915 => '\n",
      "29879 => s\n",
      "11780 => requirements\n",
      "29889 => .\n",
      "13 => \n",
      "\n",
      "22550 => Answer\n",
      "445 => this\n",
      "1494 => following\n",
      "15278 => instruction\n",
      "29901 => :\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "539 =>      \n",
      "13 => \n",
      "\n",
      "29908 => \"\n",
      "6113 => Write\n",
      "263 => a\n",
      "317 => S\n",
      "862 => par\n",
      "2239 => QL\n",
      "2346 => query\n",
      "304 => to\n",
      "1284 => find\n",
      "278 => the\n",
      "2246 => top\n",
      "29871 => \n",
      "29896 => 1\n",
      "29900 => 0\n",
      "4426 => properties\n",
      "393 => that\n",
      "4511 => connect\n",
      "1370 => war\n",
      "28739 => riors\n",
      "470 => or\n",
      "9121 => military\n",
      "24127 => personnel\n",
      "304 => to\n",
      "9121 => military\n",
      "25700 => organizations\n",
      "297 => in\n",
      "3772 => Wik\n",
      "333 => id\n",
      "532 => ata\n",
      "29892 => ,\n",
      "10372 => ordered\n",
      "491 => by\n",
      "10868 => frequency\n",
      "310 => of\n",
      "8744 => usage\n",
      "1213 => .\"\n",
      "13 => \n",
      "\n",
      "29961 => [\n",
      "29914 => /\n",
      "25580 => INST\n",
      "29962 => ]\n",
      "518 => [\n",
      "29879 => s\n",
      "862 => par\n",
      "1519 => ql\n",
      "29962 => ]\n",
      "15094 => PRE\n",
      "25634 => FIX\n",
      "289 => b\n",
      "29881 => d\n",
      "29901 => :\n",
      "529 => <\n",
      "1124 => http\n",
      "597 => ://\n",
      "1636 => www\n",
      "29889 => .\n",
      "3752 => big\n",
      "1272 => data\n",
      "29889 => .\n",
      "510 => com\n",
      "29914 => /\n",
      "29878 => r\n",
      "2176 => df\n",
      "29937 => #\n",
      "29958 => >\n",
      "13 => \n",
      "\n",
      "15094 => PRE\n",
      "25634 => FIX\n",
      "13182 => hint\n",
      "29901 => :\n",
      "529 => <\n",
      "1124 => http\n",
      "597 => ://\n",
      "1636 => www\n",
      "29889 => .\n",
      "3752 => big\n",
      "1272 => data\n",
      "29889 => .\n",
      "510 => com\n",
      "29914 => /\n",
      "1972 => query\n",
      "29950 => H\n",
      "9466 => ints\n",
      "29937 => #\n",
      "29958 => >\n",
      "13 => \n",
      "\n",
      "15094 => PRE\n",
      "25634 => FIX\n",
      "281 => w\n",
      "29881 => d\n",
      "29901 => :\n",
      "529 => <\n",
      "1124 => http\n",
      "597 => ://\n",
      "1636 => www\n",
      "29889 => .\n",
      "2851 => wik\n",
      "333 => id\n",
      "532 => ata\n",
      "29889 => .\n",
      "990 => org\n",
      "29914 => /\n",
      "10041 => entity\n",
      "3779 => />\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "268 =>    \n",
      "13 => \n",
      "\n",
      "15094 => PRE\n",
      "25634 => FIX\n",
      "281 => w\n",
      "6008 => dt\n",
      "29901 => :\n",
      "529 => <\n",
      "1124 => http\n",
      "597 => ://\n",
      "1636 => www\n",
      "29889 => .\n",
      "2851 => wik\n",
      "333 => id\n",
      "532 => ata\n",
      "29889 => .\n",
      "990 => org\n",
      "29914 => /\n",
      "7728 => prop\n",
      "29914 => /\n",
      "11851 => direct\n",
      "3779 => />\n",
      "13 => \n",
      "\n",
      "15094 => PRE\n",
      "25634 => FIX\n",
      "281 => w\n",
      "638 => ik\n",
      "747 => ib\n",
      "559 => ase\n",
      "29901 => :\n",
      "529 => <\n",
      "1124 => http\n",
      "597 => ://\n",
      "2851 => wik\n",
      "16912 => iba\n",
      "29889 => .\n",
      "344 => se\n",
      "29914 => /\n",
      "609 => ont\n",
      "3002 => ology\n",
      "29937 => #\n",
      "29958 => >\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "1678 =>   \n",
      "13 => \n",
      "\n",
      "462 =>                \n",
      "462 =>                \n",
      "632 =>            \n",
      "13 => \n",
      "\n",
      "6404 => SELECT\n",
      "1577 => ?\n",
      "2798 => count\n",
      "1577 => ?\n",
      "6799 => property\n",
      "1577 => ?\n",
      "6799 => property\n",
      "4775 => Label\n",
      "1577 => ?\n",
      "29879 => s\n",
      "1160 => amp\n",
      "5754 => WHERE\n",
      "426 => {\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "632 =>            \n",
      "13 => \n",
      "\n",
      "29912 => {\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "4706 =>       \n",
      "13 => \n",
      "\n",
      "6404 => SELECT\n",
      "313 => (\n",
      "18736 => COUNT\n",
      "10780 => (?\n",
      "667 => item\n",
      "29897 => )\n",
      "3339 => AS\n",
      "1577 => ?\n",
      "2798 => count\n",
      "29897 => )\n",
      "1577 => ?\n",
      "6799 => property\n",
      "313 => (\n",
      "8132 => SA\n",
      "3580 => MP\n",
      "1307 => LE\n",
      "10780 => (?\n",
      "667 => item\n",
      "29897 => )\n",
      "3339 => AS\n",
      "1577 => ?\n",
      "29879 => s\n",
      "1160 => amp\n",
      "29897 => )\n",
      "5754 => WHERE\n",
      "426 => {\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "539 =>      \n",
      "13 => \n",
      "\n",
      "29882 => h\n",
      "524 => int\n",
      "29901 => :\n",
      "3010 => Query\n",
      "13182 => hint\n",
      "29901 => :\n",
      "20640 => optim\n",
      "3950 => izer\n",
      "376 => \"\n",
      "8516 => None\n",
      "1642 => \".\n",
      "259 =>  \n",
      "13 => \n",
      "\n",
      "29973 => ?\n",
      "990 => org\n",
      "29918 => _\n",
      "1990 => class\n",
      "281 => w\n",
      "6008 => dt\n",
      "10834 => :[\n",
      "6799 => property\n",
      "29901 => :\n",
      "1491 => sub\n",
      "1990 => class\n",
      "310 => of\n",
      "14178 => ]*\n",
      "281 => w\n",
      "29881 => d\n",
      "10834 => :[\n",
      "10041 => entity\n",
      "29901 => :\n",
      "279 => ar\n",
      "2168 => med\n",
      "13013 => organization\n",
      "1822 => ].\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "965 =>          \n",
      "13 => \n",
      "\n",
      "29973 => ?\n",
      "990 => org\n",
      "281 => w\n",
      "6008 => dt\n",
      "10834 => :[\n",
      "6799 => property\n",
      "29901 => :\n",
      "8758 => instance\n",
      "310 => of\n",
      "29962 => ]\n",
      "29973 => ?\n",
      "1577 => ?\n",
      "990 => org\n",
      "29918 => _\n",
      "1990 => class\n",
      "869 => .\n",
      "13 => \n",
      "\n",
      "29973 => ?\n",
      "667 => item\n",
      "1577 => ?\n",
      "7728 => prop\n",
      "1577 => ?\n",
      "990 => org\n",
      "869 => .\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "539 =>      \n",
      "13 => \n",
      "\n",
      "29973 => ?\n",
      "667 => item\n",
      "281 => w\n",
      "6008 => dt\n",
      "10834 => :[\n",
      "6799 => property\n",
      "29901 => :\n",
      "26601 => occupation\n",
      "29962 => ]\n",
      "1577 => ?\n",
      "12154 => role\n",
      "869 => .\n",
      "13 => \n",
      "\n",
      "29973 => ?\n",
      "12154 => role\n",
      "281 => w\n",
      "6008 => dt\n",
      "10834 => :[\n",
      "6799 => property\n",
      "29901 => :\n",
      "1491 => sub\n",
      "1990 => class\n",
      "310 => of\n",
      "14178 => ]*\n",
      "281 => w\n",
      "29881 => d\n",
      "10834 => :[\n",
      "10041 => entity\n",
      "29901 => :\n",
      "4495 => war\n",
      "13479 => rior\n",
      "29962 => ]\n",
      "869 => .\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "3986 =>         \n",
      "13 => \n",
      "\n",
      "29973 => ?\n",
      "6799 => property\n",
      "281 => w\n",
      "638 => ik\n",
      "747 => ib\n",
      "559 => ase\n",
      "29901 => :\n",
      "11851 => direct\n",
      "29907 => C\n",
      "8342 => laim\n",
      "1577 => ?\n",
      "7728 => prop\n",
      "869 => .\n",
      "13 => \n",
      "\n",
      "29913 => }\n",
      "15345 => GROUP\n",
      "6770 => BY\n",
      "1577 => ?\n",
      "6799 => property\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "632 =>            \n",
      "13 => \n",
      "\n",
      "29913 => }\n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "462 =>                \n",
      "18884 =>               \n",
      "13 => \n",
      "\n",
      "6304 => SER\n",
      "19059 => VICE\n",
      "281 => w\n",
      "638 => ik\n",
      "747 => ib\n",
      "559 => ase\n",
      "29901 => :\n",
      "1643 => label\n",
      "426 => {\n",
      "289 => b\n",
      "29881 => d\n",
      "29901 => :\n",
      "5509 => service\n",
      "4736 => Param\n",
      "281 => w\n",
      "638 => ik\n",
      "747 => ib\n",
      "559 => ase\n",
      "29901 => :\n",
      "11675 => language\n",
      "14704 => \"[\n",
      "20656 => AUT\n",
      "29949 => O\n",
      "29918 => _\n",
      "29931 => L\n",
      "19453 => ANG\n",
      "29965 => U\n",
      "10461 => AGE\n",
      "1402 => ],\n",
      "264 => en\n",
      "1642 => \".\n",
      "500 => }\n",
      "13 => \n",
      "\n",
      "29913 => }\n",
      "462 =>                \n",
      "462 =>                \n",
      "29871 => \n",
      "13 => \n",
      "\n",
      "22364 => ORDER\n",
      "6770 => BY\n",
      "23050 => DESC\n",
      "22308 => (?\n",
      "2798 => count\n",
      "9601 => )[\n",
      "29914 => /\n",
      "29879 => s\n",
      "862 => par\n",
      "1519 => ql\n",
      "29962 => ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt)[\"input_ids\"]\n",
    "list(map(lambda x: print(f\"{x} => {tokenizer.decode(x)}\"), input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[29961, 29914, 25580, 29962]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
