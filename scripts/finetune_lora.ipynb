{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup the model\n",
    "checkpoint = \"bigscience/bloomz-560m\"\n",
    "# checkpoint = \"bigscience/bigscience-small-testing\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto') # load_in_8bit=True,\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Freeze the original weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False # freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable() # reduce the number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 560787456 || trainable%: 0.2804741766549072\n"
     ]
    }
   ],
   "source": [
    "# 3) Setting up the LoRA Adapters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100*trainable_params/all_param}\")\n",
    "    \n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, # attention heads\n",
    "    lora_alpha=32, # alpha scaling TODO: Research what this is\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/Alexis Strappazzon/.cache/huggingface/datasets/json/cleaned_queries-4d88e17e06e65763/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e627ea1e8bed4610b8a25ed4d7872058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4) Data\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(path=\"./datasets/cleaned_queries/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SELECT ?property ?propertyType ?propertyLabel ?propertyDescription WHERE {\\n?property wikibase:propertyType ?propertyType .\\nSERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n} ORDER BY ASC(xsd:integer(STRAFTER(STR(?property), \\'P\\')))',\n",
       " 'SELECT ?id ?idLabel ?idDescription ?new{\\n?id wikibase:directClaim ?pid .\\nminus{?id wikibase:propertyType wikibase:ExternalId}\\nBIND(Replace(STR(?id),\"http://www.wikidata.org/entity/P\",\" \") as ?new)\\nSERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\\n}\\nORDER BY DESC(xsd:integer(?new))',\n",
       " 'SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?article wdt:P31/wdt:P279* wd:Q13442814\\n}',\n",
       " 'SELECT (COUNT(DISTINCT ?article) AS ?count)\\nWHERE {?article wdt:P31/wdt:P279* wd:Q95074}',\n",
       " 'SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item wdt:P625 [] }']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['query'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': 'Counting stuff on Wikidata\\nAll Wikidata properties with label and description, ordered numerically\\nAdapted from one of the Query Service Examples',\n",
       "  'description': 'Wikidata properties in numerical order'},\n",
       " {'context': 'Counting stuff on Wikidata\\nVariation of the above excluding external IDs (thanks Magnus Salgo)',\n",
       "  'description': 'Wikidata properties excluding external IDs'},\n",
       " {'context': 'Counting stuff on Wikidata\\nCount of scientific articles',\n",
       "  'description': ''},\n",
       " {'context': 'Counting stuff on Wikidata\\nCount of fictional characters',\n",
       "  'description': 'Count of fictional characters'},\n",
       " {'context': 'Counting stuff on Wikidata\\nCount of items with coordinate locations',\n",
       "  'description': 'Count of items with coordinate locations'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"]['metadata'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b0bb6098a842cd8bd2919d688f2361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Counting stuff on Wikidata\\nAll Wikidata properties with label and description, ordered numerically\\nAdapted from one of the Query Service Examples\\nWikidata properties in numerical order ->: SELECT ?property ?propertyType ?propertyLabel ?propertyDescription WHERE {\\n?property wikibase:propertyType ?propertyType .\\nSERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n} ORDER BY ASC(xsd:integer(STRAFTER(STR(?property), \\'P\\')))',\n",
       " 'Counting stuff on Wikidata\\nVariation of the above excluding external IDs (thanks Magnus Salgo)\\nWikidata properties excluding external IDs ->: SELECT ?id ?idLabel ?idDescription ?new{\\n?id wikibase:directClaim ?pid .\\nminus{?id wikibase:propertyType wikibase:ExternalId}\\nBIND(Replace(STR(?id),\"http://www.wikidata.org/entity/P\",\" \") as ?new)\\nSERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\\n}\\nORDER BY DESC(xsd:integer(?new))',\n",
       " 'Counting stuff on Wikidata\\nCount of scientific articles\\n ->: SELECT (COUNT(?article) AS ?count)\\nWHERE {\\n?article wdt:P31/wdt:P279* wd:Q13442814\\n}',\n",
       " 'Counting stuff on Wikidata\\nCount of fictional characters\\nCount of fictional characters ->: SELECT (COUNT(DISTINCT ?article) AS ?count)\\nWHERE {?article wdt:P31/wdt:P279* wd:Q95074}',\n",
       " 'Counting stuff on Wikidata\\nCount of items with coordinate locations\\nCount of items with coordinate locations ->: SELECT (COUNT(?item) AS ?count)\\nWHERE { ?item wdt:P625 [] }']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_columns(example):\n",
    "    example[\"prediction\"] = example[\"metadata\"][\"context\"] + \"\\n\" + example[\"metadata\"][\"description\"] + \" ->: \" + str(example[\"query\"])\n",
    "    return example\n",
    "\n",
    "data[\"train\"] = data[\"train\"].map(merge_columns)\n",
    "data[\"train\"][\"prediction\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'SELECT ?property ?propertyType ?propertyLabel ?propertyDescription WHERE {\\n?property wikibase:propertyType ?propertyType .\\nSERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n} ORDER BY ASC(xsd:integer(STRAFTER(STR(?property), \\'P\\')))',\n",
       " 'metadata': {'context': 'Counting stuff on Wikidata\\nAll Wikidata properties with label and description, ordered numerically\\nAdapted from one of the Query Service Examples',\n",
       "  'description': 'Wikidata properties in numerical order'},\n",
       " 'prediction': 'Counting stuff on Wikidata\\nAll Wikidata properties with label and description, ordered numerically\\nAdapted from one of the Query Service Examples\\nWikidata properties in numerical order ->: SELECT ?property ?propertyType ?propertyLabel ?propertyDescription WHERE {\\n?property wikibase:propertyType ?propertyType .\\nSERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n} ORDER BY ASC(xsd:integer(STRAFTER(STR(?property), \\'P\\')))'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516dc6bc6b8c4d9c85aa2f031a51372e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_processed = data.map(lambda samples: tokenizer(samples['prediction']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'metadata', 'prediction', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 6739\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SparQLDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexis Strappazzon\\Documents\\git\\llm-lora-finetunning-sparql\\.venv\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e8dc8b3645493f8cfde7e70ace8b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1955, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 4.7607, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 4.2144, 'learning_rate': 6e-06, 'epoch': 0.01}\n",
      "{'loss': 5.26, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}\n",
      "{'loss': 4.6033, 'learning_rate': 1e-05, 'epoch': 0.01}\n",
      "{'loss': 4.6875, 'learning_rate': 1.2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.5105, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6486, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7938, 'learning_rate': 1.8e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7488, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8069, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8849, 'learning_rate': 2.4e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3017, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m\n\u001b[0;32m      3\u001b[0m trainer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mTrainer(\n\u001b[0;32m      4\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      5\u001b[0m     train_dataset\u001b[39m=\u001b[39mdata_processed[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     data_collator\u001b[39m=\u001b[39mtransformers\u001b[39m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Alexis Strappazzon\\Documents\\git\\llm-lora-finetunning-sparql\\.venv\\lib\\site-packages\\transformers\\trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1661\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1663\u001b[0m )\n\u001b[1;32m-> 1664\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1665\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1666\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1667\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1668\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1669\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Alexis Strappazzon\\Documents\\git\\llm-lora-finetunning-sparql\\.venv\\lib\\site-packages\\transformers\\trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1938\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1940\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1942\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1943\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1944\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1945\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1946\u001b[0m ):\n\u001b[0;32m   1947\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Alexis Strappazzon\\Documents\\git\\llm-lora-finetunning-sparql\\.venv\\lib\\site-packages\\transformers\\trainer.py:2745\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2742\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m   2744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m-> 2745\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   2746\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_apex:\n\u001b[0;32m   2747\u001b[0m     \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mscale_loss(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer) \u001b[39mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[1;32mc:\\Users\\Alexis Strappazzon\\Documents\\git\\llm-lora-finetunning-sparql\\.venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Alexis Strappazzon\\Documents\\git\\llm-lora-finetunning-sparql\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5) Training\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data_processed['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2, # number of example per batch\n",
    "        gradient_accumulation_steps=8, # number of batch to see before applying gradient update -> per_device_train_batch_size * gradient_accumulation_steps = number of example seen before an update\n",
    "        warmup_steps=100, # starts with a very low lr and linearly goes up to the target lr every steps\n",
    "        max_steps=10000, # number of steps after which the traning stops\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Push model to hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.push_to_hub(user/repoid,\n",
    "                  use_auth_token=True,\n",
    "                  commit_message=\"basic training\",\n",
    "                  private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"username/repoid\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, device_map=\"auto\") # load_in_8bits=True,\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 86753,   7508,    386,  20038,   1002,    426, 108045,    530,   9810,\n",
      "          14062,    632,  35847,    982,  11953,     29,    210]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "\n",
      " “Training models with PEFT and LoRA is cool” ->:                                                   \n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer('“Training models with PEFT and LoRA is cool” ->: ', return_tensors='pt').to(model.device)\n",
    "print(batch)\n",
    "model.eval()\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],  max_new_tokens=50)\n",
    "    \n",
    "print(\"\\n\\n\", tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
